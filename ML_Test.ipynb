{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    \"\"\"\n",
    "    Process new data and make predictions using saved models\n",
    "    \n",
    "    Args:\n",
    "        acquiring_path: Path to new acquiring companies CSV\n",
    "        acquired_path: Path to new acquired companies CSV  \n",
    "        acquisitions_path: Path to new acquisitions CSV\n",
    "        output_path: Where to save predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================================\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    # ===================================\n",
    "    \n",
    "    # Load acquiring company transformers\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "        \n",
    "    # Load acquired company transformers\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    with open('label_encoders_acquired.pkl', 'rb') as f:\n",
    "        label_encoders_acquired = pickle.load(f)\n",
    "        \n",
    "    # Load acquisitions transformers\n",
    "    with open('ohe_acquisitions.pkl', 'rb') as f:\n",
    "        ohe_acquisitions = pickle.load(f)\n",
    "        \n",
    "    # Load final preprocessing objects\n",
    "    with open('final_imputer.pkl', 'rb') as f:\n",
    "        final_imputer = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "        \n",
    "    # Load model\n",
    "    with open('final_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # ==============================\n",
    "    # Preprocess New Data (Same as Training)\n",
    "    # ==============================\n",
    "    \n",
    "    # Process each dataset with saved transformers\n",
    "    def process_acquiring_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Apply same cleaning as training\n",
    "        data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'], \n",
    "                 axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        \n",
    "        # Use mean from training (would need to save this)\n",
    "        data['Number of Employees'] = data['Number of Employees'].replace(\n",
    "            0, 450)  # Replace with saved mean from training\n",
    "        \n",
    "        # Handle IPO status\n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data.drop('IPO', axis=1, inplace=True)\n",
    "        \n",
    "        # Process Market Categories with saved MLB\n",
    "        data['Market Categories'] = data['Market Categories'].fillna('')\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories'].str.split(',')),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Process text with saved TF-IDF\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Final cleanup\n",
    "        data = pd.concat([data, category_dummies, tfidf_df], axis=1)\n",
    "        data.drop(['Market Categories', 'Tagline', 'Description', 'Text_Combined'], \n",
    "                 axis=1, inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Similar functions for acquired and acquisitions...\n",
    "    # (Implementation would mirror the training preprocessing but using saved transformers)\n",
    "    \n",
    "    # Preprocess each new dataset\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path)) \n",
    "    acquisitions_new = process_acquisitions_new(pd.read_csv(acquisitions_path))\n",
    "    \n",
    "    # Merge the new data (same as training)\n",
    "    final_new = merge_datasets(\n",
    "        acquiring_new, acquired_new, acquisitions_new, save_artifacts=False\n",
    "    )\n",
    "    \n",
    "    # Handle missing values with saved imputer\n",
    "    final_new_imputed = pd.DataFrame(\n",
    "        final_imputer.transform(final_new),\n",
    "        columns=final_new.columns\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    X_new = final_new_imputed.drop(\n",
    "        ['Deal size class', 'Acquired Company', 'Acquiring Company'], \n",
    "        axis=1, errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Apply same scaling and PCA as training\n",
    "    X_new_scaled = final_scaler.transform(X_new)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Save predictions with original data\n",
    "    final_new['Predicted_Deal_Size'] = predictions\n",
    "    final_new.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
