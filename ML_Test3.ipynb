{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698874ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "from collections import Counter\n",
    "from joblib import dump\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pycountry\n",
    "import re\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COLUMN_ALIASES = {\n",
    "    'IPO Date': 'IPO',\n",
    "    'Went Public': 'IPO',\n",
    "    'Public Listing': 'IPO'\n",
    "}\n",
    "\n",
    "def ensure_ipo(df):\n",
    "    df.rename(columns=COLUMN_ALIASES, inplace=True)\n",
    "    if 'IPO' not in df.columns:\n",
    "        df['IPO'] = np.nan\n",
    "    return df\n",
    "# ======================== CLASSES ========================\n",
    "class CorrelationFilter:\n",
    "    def __init__(self, threshold=0.85):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = set()  # Stores columns to drop\n",
    "        self.fitted = False   # Tracks if fit() was called\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identifies highly correlated columns to drop.\"\"\"\n",
    "        numerical_data = data.select_dtypes(include=[np.number])\n",
    "        corr_matrix = numerical_data.corr()\n",
    "\n",
    "        self.to_drop = set()  # Reset in case fit() is called again\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    self.to_drop.add(colname)\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Drops columns identified in fit().\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "        \n",
    "        cols_to_drop = list(self.to_drop & set(data.columns))\n",
    "        return data.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Combines fit() and transform().\"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def get_columns_to_drop(self):\n",
    "        \"\"\"Returns the list of columns to be dropped.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() first!\")\n",
    "        return list(self.to_drop)\n",
    "    \n",
    "class CategoryReducer:\n",
    "    def __init__(self, category_columns, top_n=15):\n",
    "        self.category_columns = category_columns\n",
    "        self.top_n = top_n\n",
    "        self.top_categories = None  # Will store the top categories from training\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Identify and store the top N categories (only during training)\n",
    "        self.top_categories = (\n",
    "            data[self.category_columns]\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(self.top_n)\n",
    "            .index.tolist()\n",
    "        )\n",
    "        return self  # For sklearn compatibility\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.top_categories is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        # Keep only the top categories (from training)\n",
    "        df_top = data[self.top_categories].copy()\n",
    "\n",
    "        # Sum remaining categories into \"Other\"\n",
    "        other_columns = list(set(self.category_columns) - set(self.top_categories))\n",
    "        df_top['Other'] = data[other_columns].sum(axis=1).clip(upper=1)  # Ensures 0 or 1\n",
    "\n",
    "        # Drop original columns and concatenate reduced set\n",
    "        data = data.drop(columns=self.category_columns)\n",
    "        return pd.concat([data, df_top], axis=1)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    # Handle pickle compatibility\n",
    "    def __getstate__(self):\n",
    "        return {k: v for k, v in self.__dict__.items() \n",
    "                if k in ['category_columns', 'top_n', 'top_categories',\n",
    "                         'original_categories', 'expected_columns']}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        # Initialize missing attributes for old versions\n",
    "        if not hasattr(self, 'original_categories'):\n",
    "            self.original_categories = self.category_columns\n",
    "        if not hasattr(self, 'expected_columns'):\n",
    "            self.expected_columns = (self.top_categories + ['Other'] \n",
    "                                     if self.top_categories else None)\n",
    "class AgeTransformer:\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "        self.age_mode = None\n",
    "        self.column_exists = True  # Track if column existed during training\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Check if column exists in training data\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            self.column_exists = False\n",
    "            return self\n",
    "            \n",
    "        data = data.copy()\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        mode_series = data['age'].mode()\n",
    "        self.age_mode = mode_series[0] if not mode_series.empty else 5\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Handle missing 'Year Founded' column gracefully\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Create column if it doesn't exist\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            if self.column_exists:\n",
    "                # Column existed in training but missing in new data\n",
    "                data['Year Founded'] = np.nan\n",
    "            else:\n",
    "                # Column never existed (new scenario)\n",
    "                data['age'] = self.age_mode\n",
    "                return data\n",
    "                \n",
    "        # Original processing if column exists\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        data.drop(columns=['Year Founded'], inplace=True, errors='ignore')\n",
    "        data['age'].fillna(self.age_mode, inplace=True)\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class IPOAgeTransformer:\n",
    "    def __init__(self, current_year=2025, unknown_placeholder=\"Unknown\"):\n",
    "        self.current_year = current_year\n",
    "        self.unknown_placeholder = unknown_placeholder  # Replace NaN values\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Stateless (no training needed). For pipeline compatibility.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Computes IPO age and replaces missing values.\"\"\"\n",
    "        # if 'IPO' not in data.columns:\n",
    "        #     raise ValueError(\"Column 'IPO' not found in data.\")\n",
    "\n",
    "        df = data.copy()\n",
    "        df = ensure_ipo(df)\n",
    "        df['IPO'] = pd.to_numeric(df['IPO'], errors='coerce')\n",
    "        \n",
    "        # Compute age (clamp negative values to 0)\n",
    "        df['age IPO'] = (self.current_year - df['IPO']).clip(lower=0)\n",
    "        \n",
    "        # Replace missing ages with placeholder\n",
    "        df['age IPO'] = df['age IPO'].replace(\n",
    "            np.nan, self.unknown_placeholder\n",
    "        )\n",
    "        \n",
    "        # Drop original IPO column\n",
    "        df.drop(columns=['IPO'], inplace=True, errors='ignore')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.transform(data)  # fit() is stateless\n",
    "    \n",
    "class EmployeeDataCleaner:\n",
    "    def __init__(self):\n",
    "        self.employee_mode = None\n",
    "        self.mean_without_zeros = None\n",
    "        self.fitted = False  # Safety flag\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Compute and store statistics from training data.\"\"\"\n",
    "        # Validate columns\n",
    "        required_columns = [\n",
    "            'Number of Employees (year of last update)',\n",
    "            'Number of Employees'\n",
    "        ]\n",
    "        for col in required_columns:\n",
    "            if col not in data.columns:\n",
    "                raise ValueError(f\"Column '{col}' not found in data.\")\n",
    "\n",
    "        # Compute mode for 'Number of Employees (year of last update)'\n",
    "        mode_series = data['Number of Employees (year of last update)'].mode()\n",
    "        self.employee_mode = mode_series[0] if not mode_series.empty else 0\n",
    "\n",
    "        # Compute mean (excluding zeros/negatives) for 'Number of Employees'\n",
    "        non_zero_employees = data.loc[\n",
    "            data['Number of Employees'] > 0, 'Number of Employees'\n",
    "        ]\n",
    "        self.mean_without_zeros = non_zero_employees.mean()\n",
    "\n",
    "        # Fallback if all values are zero/NaN\n",
    "        if pd.isna(self.mean_without_zeros):\n",
    "            self.mean_without_zeros = data['Number of Employees'].median()  # or a global default\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Apply cleaning using statistics from fit().\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "\n",
    "        # Fill missing values with training mode\n",
    "        if 'Number of Employees (year of last update)' in df.columns:\n",
    "            df['Number of Employees (year of last update)'].fillna(\n",
    "                self.employee_mode, inplace=True\n",
    "            )\n",
    "\n",
    "        # Handle nulls/negatives and replace zeros with training mean\n",
    "        if 'Number of Employees' in df.columns:\n",
    "            df['Number of Employees'] = np.where(\n",
    "                df['Number of Employees'].isna() | (df['Number of Employees'] < 0),\n",
    "                0,\n",
    "                df['Number of Employees']\n",
    "            )\n",
    "            df['Number of Employees'] = df['Number of Employees'].replace(\n",
    "                0, self.mean_without_zeros\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class BoardMembersTransformer:\n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count  # Keep only members appearing ≥ min_count times\n",
    "        self.common_members = None  # Stores frequent members from training\n",
    "        self.member_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring board members from training data.\"\"\"\n",
    "        if 'Board Members' not in data.columns:\n",
    "            raise ValueError(\"Column 'Board Members' not found in data.\")\n",
    "\n",
    "        all_members = []\n",
    "        for cell in data['Board Members'].dropna():\n",
    "            members = [name.strip() for name in str(cell).split(',')]\n",
    "            all_members.extend(members)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.member_counts_ = Counter(all_members)\n",
    "        self.common_members = {\n",
    "            name for name, count in self.member_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert board members into binary features for common members.\"\"\"\n",
    "        if self.common_members is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common member\n",
    "        for member in self.common_members:\n",
    "            df[f'Board Member: {member}'] = df['Board Members'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and member in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature (total members or binary \"has members\")\n",
    "        df['Has Board Members'] = df['Board Members'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Board Members'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class BoardMembersTransformer:\n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count  # Keep only members appearing ≥ min_count times\n",
    "        self.common_members = None  # Stores frequent members from training\n",
    "        self.member_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring board members from training data.\"\"\"\n",
    "        if 'Board Members' not in data.columns:\n",
    "            raise ValueError(\"Column 'Board Members' not found in data.\")\n",
    "\n",
    "        all_members = []\n",
    "        for cell in data['Board Members'].dropna():\n",
    "            members = [name.strip() for name in str(cell).split(',')]\n",
    "            all_members.extend(members)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.member_counts_ = Counter(all_members)\n",
    "        self.common_members = {\n",
    "            name for name, count in self.member_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert board members into binary features for common members.\"\"\"\n",
    "        if self.common_members is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common member\n",
    "        for member in self.common_members:\n",
    "            df[f'Board Member: {member}'] = df['Board Members'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and member in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature (total members or binary \"has members\")\n",
    "        df['Has Board Members'] = df['Board Members'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Board Members'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class FoundersTransformer:\n",
    "    def __init__(self, min_count=3):\n",
    "        self.min_count = min_count  # Keep founders appearing ≥ min_count times\n",
    "        self.common_founders = None  # Stores frequent founders from training\n",
    "        self.founder_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring founders from training data.\"\"\"\n",
    "        if 'Founders' not in data.columns:\n",
    "            raise ValueError(\"Column 'Founders' not found in data.\")\n",
    "\n",
    "        all_founders = []\n",
    "        for cell in data['Founders'].dropna():\n",
    "            founders = [name.strip() for name in str(cell).split(',')]\n",
    "            all_founders.extend(founders)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.founder_counts_ = Counter(all_founders)\n",
    "        self.common_founders = {\n",
    "            name for name, count in self.founder_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert founders into binary features for common founders.\"\"\"\n",
    "        if self.common_founders is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common founder\n",
    "        for founder in self.common_founders:\n",
    "            df[f'Founder: {founder}'] = df['Founders'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and founder in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature\n",
    "        df['Has Founders'] = df['Founders'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Founders'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class TaglineCategoryGuesser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_keywords = {'Artificial Intelligence': ['ai', 'machine learning', 'deep learning', 'neural network'], 'Mobile': ['mobile', 'android', 'ios', 'app store', 'smartphone'], 'E-Commerce': ['ecommerce', 'e-commerce', 'shopping', 'online store'], 'FinTech': ['finance', 'banking', 'payments', 'fintech', 'crypto', 'blockchain'], 'Healthcare': ['health', 'medical', 'hospital', 'doctor', 'pharma'], 'Social Media': ['social network', 'community', 'messaging', 'chat'], 'Gaming': ['game', 'gaming', 'video game', 'esports'], 'Cloud': ['cloud', 'saas', 'paas', 'infrastructure'], 'EdTech': ['education', 'learning', 'students', 'teaching', 'school'], 'Data Analytics': ['analytics', 'data science', 'big data', 'insights']}\n",
    "\n",
    "    def guess_category_from_tagline(self, tagline):\n",
    "        tagline = str(tagline).lower()\n",
    "        matched = [cat for (cat, keywords) in self.category_keywords.items() if any((keyword in tagline for keyword in keywords))]\n",
    "        if len(matched) == 0:\n",
    "            matched = ['Software', 'Advertising']\n",
    "        elif len(matched) == 1:\n",
    "            matched.append('Software')\n",
    "        return ', '.join(matched)\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['Tagline'] = df['Tagline'].fillna('')\n",
    "        df['Market Categories'] = df['Market Categories'].fillna('Unknown')\n",
    "        df['Market Categories'] = df.apply(lambda row: self.guess_category_from_tagline(row['Tagline']) if str(row['Market Categories']).strip().lower() in ['unknown', 'nan', 'none', ''] else row['Market Categories'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class MarketCategoryGeneralizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_mapping = {'Software': 'Technology & Software', 'Advertising': 'Advertising & Marketing', 'E-Commerce': 'E-Commerce & Online Services', 'Mobile': 'Mobile & Consumer Electronics', 'Games': 'Games & Entertainment', 'Social Media': 'Social Networking & Communication', 'Cloud': 'Technology & Software', 'Finance': 'Finance & Payments', 'Healthcare': 'Healthcare & Wellness', 'Semiconductors': 'Technology Hardware', 'Data Analytics': 'Analytics & Data Science', 'Search': 'Advertising & Marketing', 'Video': 'Games & Entertainment', 'Networking': 'Telecom & Networks', 'Messaging': 'Social Networking & Communication', 'Education': 'Education & Learning', 'News': 'Media & News', 'Photo Sharing': 'Digital Media & Content', 'Mobile Payments': 'Finance & Payments', 'Robotics': 'Games & Entertainment', 'Music': 'Games & Entertainment', 'Photo Editing': 'Digital Media & Content', 'Online Rental': 'E-Commerce & Online Services', 'Location Based Services': 'Telecom & Networks', 'Enterprise Software': 'Technology & Software', 'Video Streaming': 'Games & Entertainment', 'PaaS': 'Technology & Software', 'SaaS': 'Technology & Software', 'Health and Wellness': 'Healthcare & Wellness', 'Web Hosting': 'Technology & Software', 'Internet of Things': 'IoT (Internet of Things)', 'Cloud Security': 'Technology & Software', 'Virtual Currency': 'Finance & Payments', 'Search Marketing': 'Advertising & Marketing', 'Mobile Social': 'Social Networking & Communication', 'Retail': 'Retail & Fashion', 'Consulting': 'Others & Miscellaneous', 'Aerospace': 'Others & Miscellaneous', 'Food Delivery': 'Consumer Goods & Services', 'Fashion': 'Retail & Fashion', 'Wine And Spirits': 'Consumer Goods & Services', 'Streaming': 'Games & Entertainment', 'Task Management': 'Others & Miscellaneous', 'Video Chat': 'Social Networking & Communication', 'Personalization': 'Advertising & Marketing', 'Shopping': 'E-Commerce & Online Services', 'Local': 'E-Commerce & Online Services', 'News': 'Media & News', 'Fraud Detection': 'Advertising & Marketing', 'Image Recognition': 'Technology Hardware', 'Virtualization': 'Games & Entertainment', 'Analytics': 'Analytics & Data Science', 'Video on Demand': 'Games & Entertainment', 'Mobile Payments': 'Finance & Payments', 'Marketing Automation': 'Advertising & Marketing', 'Consumer Electronics': 'Mobile & Consumer Electronics', 'Video Games': 'Games & Entertainment', 'Public Relations': 'Advertising & Marketing'}\n",
    "\n",
    "    def map_categories(self, row):\n",
    "        categories = str(row).split(',')\n",
    "        generalized = []\n",
    "        for cat in categories:\n",
    "            cat = cat.strip()\n",
    "            if cat in self.category_mapping:\n",
    "                generalized.append(self.category_mapping[cat])\n",
    "            else:\n",
    "                generalized.append('Others & Miscellaneous')\n",
    "        return ', '.join(set(generalized))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df['Generalized Market Categories'] = df['Market Categories'].fillna('').apply(self.map_categories)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CountryRegionFiller:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.countries = [country.name for country in pycountry.countries]\n",
    "        self.regions = ['California', 'New York', 'Texas', 'Basel', 'Utah', 'Île-de-France', 'Bavaria', 'Ontario', 'Switzerland', 'United States', 'France', 'Great Britain', 'Israel', 'Sweden', 'Canada', 'Germany', 'Japan', 'India', 'Denmark', 'China', 'Spain', 'Netherlands', 'Finland', 'Australia', 'Ireland', 'United Stats of AMerica', 'United Arab Emirates', 'Quebec']\n",
    "\n",
    "    def find_place(self, text, place_list):\n",
    "        for place in place_list:\n",
    "            if re.search('\\\\b' + re.escape(place) + '\\\\b', str(text)):\n",
    "                return place\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for (idx, row) in df[df['Country (HQ)'].isnull() | df['State / Region (HQ)'].isnull()].iterrows():\n",
    "            desc = row['Description']\n",
    "            if pd.isnull(desc):\n",
    "                continue\n",
    "            country = self.find_place(desc, self.countries)\n",
    "            region = self.find_place(desc, self.regions)\n",
    "            if pd.isnull(row['Country (HQ)']) and country:\n",
    "                df.at[idx, 'Country (HQ)'] = country\n",
    "            if pd.isnull(row['State / Region (HQ)']) and region:\n",
    "                df.at[idx, 'State / Region (HQ)'] = region\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "class CategoricalFillerAndEncoder:\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.modes = {}\n",
    "        self.label_encoders = {}\n",
    "        self.label_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            mode_val = X[col].mode()[0]\n",
    "            self.modes[col] = mode_val\n",
    "            le = LabelEncoder()\n",
    "            filled = X[col].fillna(mode_val).astype(str)\n",
    "            le.fit(filled)\n",
    "            self.label_encoders[col] = le\n",
    "            self.label_maps[col] = {label: i for (i, label) in enumerate(le.classes_)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            mode_val = self.modes[col]\n",
    "            label_map = self.label_maps[col]\n",
    "            df[col] = df[col].fillna(mode_val).astype(str)\n",
    "            df[col + '_LabelEncoded'] = df[col].map(lambda x: label_map.get(x, -1))\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status_cols_ = None\n",
    "        self.terms_cols_ = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.status_cols_ = df['Status'].unique()\n",
    "        self.terms_cols_ = df['Terms'].unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df = pd.get_dummies(df, columns=['Status'], drop_first=False)\n",
    "        df = pd.get_dummies(df, columns=['Terms'], drop_first=False)\n",
    "        if 'Terms_Cash, Stock' in df.columns:\n",
    "            cash_stock_mask = df['Terms_Cash, Stock'] == 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
    "            df = df.drop('Terms_Cash, Stock', axis=1)\n",
    "        expected_cols = [f'Status_{s}' for s in self.status_cols_] + [f'Terms_{t}' for t in self.terms_cols_ if t != 'Cash, Stock']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df[expected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_409652/4037191763.py:515: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
      "/tmp/ipykernel_409652/4037191763.py:516: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) [' Advertising Platforms', ' All Markets', ' All Students', ' Auctions', ' Blogging Platforms', ' Cloud Computing', ' Colleges', ' Communities', ' Consulting', ' Consumer Electronics', ' Consumer Goods', ' Content Creators', ' Creative', ' Crowdsourcing', ' Curated Web', ' Design', ' Digital Media', ' E-Commerce', ' Email', ' Enterprise Software', ' Enterprises', ' Facebook Applications', ' Hardware', ' Hardware + Software', ' Identity', ' Image Recognition', ' Information Technology', ' MicroBlogging', ' Mobile', ' Networking', ' Photography', ' RIM', ' SMS', ' Search', ' Security', ' Social Bookmarking', ' Social Media', ' Software', ' Storage', ' Technology', ' Video Streaming', ' Web Hosting', ' Wireless'] will be ignored\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_409652/4037191763.py:141: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['age'].fillna(self.age_mode, inplace=True)\n",
      "/tmp/ipykernel_409652/4037191763.py:224: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Number of Employees (year of last update)'].fillna(\n",
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy vs Random: 0.0417\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    with open(\"correlation_filter.pkl\", \"rb\") as f:\n",
    "        loaded_filter = pickle.load(f)\n",
    "    with open(\"category_reducer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_reducer = pickle.load(f)\n",
    "    with open('Age_column_acquiring.pkl', 'rb') as f:\n",
    "        age_mode_col = pickle.load(f)\n",
    "    with open(\"ipo_transformer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_ipo_transformer = pickle.load(f)\n",
    "    with open(\"employee_cleaner_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_employee_cleaner = pickle.load(f)\n",
    "    with open(\"board_members_transformer.pkl\", \"rb\") as f:\n",
    "        loaded_board_transformer = pickle.load(f)\n",
    "    with open(\"founders_transformer.pkl\", \"rb\") as f:\n",
    "        loaded_founders_transformer = pickle.load(f)\n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "    with open(\"tagline_guesser_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_guesser = pickle.load(f)\n",
    "    with open(\"category_generalizer_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_generalizer = pickle.load(f)\n",
    "    with open(\"country_region_filler_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_filler = pickle.load(f)\n",
    "    with open(\"categorical_encoder_acquired.pkl\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "    with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "        modes = pickle.load(f)\n",
    "\n",
    "    # Preprocess New Data\n",
    "    def process_acquiring_new(data):\n",
    "        data = data.copy()\n",
    "        data = data.drop(['CrunchBase Profile', 'Image', 'Homepage', 'Twitter', 'API'], \n",
    "                       axis=1, errors='ignore')\n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        \n",
    "        categorical_columns = ['City (HQ)', 'State / Region (HQ)', 'Country (HQ)', \n",
    "                             'Company', 'Number of Employees (year of last update)']\n",
    "        label_encoders = {}\n",
    "        for col in categorical_columns:\n",
    "            if col in data.columns:\n",
    "                data[col] = data[col].astype(str)\n",
    "                le = LabelEncoder()\n",
    "                data[col] = le.fit_transform(data[col])\n",
    "                label_encoders[col] = le\n",
    "        \n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data = data.drop_duplicates()\n",
    "        data = data.drop('IPO', axis=1, errors='ignore')\n",
    "        data['Market Categories List'] = (\n",
    "            data['Market Categories']\n",
    "                .fillna('')\n",
    "                .str.split(',')\n",
    "        )\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories List']),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "            index=data.index\n",
    "        )\n",
    "        data = pd.concat([data, category_dummies], axis=1)\n",
    "        data = data.drop(columns=['Market Categories', 'Market Categories List'], errors='ignore')\n",
    "        data = loaded_filter.transform(data)\n",
    "        columns_to_drop_test = loaded_filter.get_columns_to_drop()\n",
    "        data = data.drop(columns=columns_to_drop_test, errors='ignore')\n",
    "        missing_original = set(loaded_reducer.original_categories) - set(data.columns)\n",
    "        for col in missing_original:\n",
    "            data[col] = 0\n",
    "        data = loaded_reducer.transform(data)\n",
    "        data = age_mode_col.transform(data)\n",
    "        data = loaded_ipo_transformer.transform(data)\n",
    "        data = data.drop(columns=['Address (HQ)'], errors='ignore')\n",
    "        data = loaded_employee_cleaner.transform(data)\n",
    "        data = loaded_board_transformer.transform(data)\n",
    "        data = loaded_founders_transformer.transform(data)\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            return text\n",
    "        data['Text_Combined'] = data['Text_Combined'].apply(clean_text)\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        data = pd.concat([data, tfidf_df], axis=1)\n",
    "        cols_to_drop = ['Tagline', 'Description', 'Text_Combined', 'Address (HQ)', \n",
    "                      'Board Members', 'Founders']\n",
    "        existing_cols = [col for col in cols_to_drop if col in data.columns]\n",
    "        data = data.drop(existing_cols, axis=1, errors='ignore')\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        return data\n",
    "\n",
    "    def process_acquired_new(data):\n",
    "        data = data.copy()\n",
    "        data = loaded_guesser.transform(data)\n",
    "        data = loaded_generalizer.transform(data)\n",
    "        data = loaded_filler.transform(data)\n",
    "        data = encoder.transform(data)\n",
    "        data['Acquired by'] = data['Acquired by'].fillna('Salesforce')\n",
    "        columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter', 'Address (HQ)', \n",
    "                         'API', 'Description', 'Tagline', 'Market Categories', \n",
    "                         'City (HQ)', 'State / Region (HQ)', 'Country (HQ)', \n",
    "                         'Generalized Market Categories', 'Year Founded']\n",
    "        data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        return data\n",
    "\n",
    "    def process_acquisitions_new(data):\n",
    "        data = data.copy()\n",
    "        cols_to_drop = [\"Acquisition Profile\", \"News\", \"News Link\"]\n",
    "        for col in data.columns:\n",
    "            if col.lower() == 'deal size class':\n",
    "                cols_to_drop.append(col)\n",
    "        data = data.drop(columns=cols_to_drop, errors='ignore')\n",
    "        with open('custom_acquisitions_encoder.pkl', 'rb') as f:\n",
    "            encoder = pickle.load(f)\n",
    "        for col, mode_val in modes.items():\n",
    "            if col in data.columns and mode_val is not None:\n",
    "                data[col] = data[col].fillna(mode_val)\n",
    "        if 'Deal announced on' in data.columns:\n",
    "            data['Deal_date'] = pd.to_datetime(data['Deal announced on'], dayfirst=True, errors='coerce')\n",
    "            data['Deal_day'] = data['Deal_date'].dt.day\n",
    "            data['Deal_month'] = data['Deal_date'].dt.month\n",
    "            data['Deal_dayofweek'] = data['Deal_date'].dt.dayofweek\n",
    "            data = data.drop(['Deal announced on', 'Deal_date'], axis=1, errors='ignore')\n",
    "        encoded_status_terms = encoder.transform(data)\n",
    "        data = pd.concat([data, encoded_status_terms], axis=1)\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        return data\n",
    "\n",
    "    # Preprocess datasets\n",
    "    acquisitions_new = pd.read_csv(acquisitions_path).reset_index(drop=True)\n",
    "    acquisitions_new = process_acquisitions_new(acquisitions_new)\n",
    "    \n",
    "    # Generate random comparison data\n",
    "    np.random.seed(42)\n",
    "    actual_deal_size = pd.Series(np.random.choice(target_encoder.classes_, size=len(acquisitions_new)))\n",
    "\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path))\n",
    "\n",
    "    # Merge datasets\n",
    "    def find_company_column(df):\n",
    "        for col in df.columns:\n",
    "            if \"company\" in col.lower():\n",
    "                return col\n",
    "        raise ValueError(\"No company name column found.\")\n",
    "    \n",
    "    final_df = acquisitions_new.copy()\n",
    "    merge_targets = [\n",
    "        ('Acquired Company', acquired_new, '_Acquired'),\n",
    "        ('Acquiring Company', acquiring_new, '_Acquiring')\n",
    "    ]\n",
    "\n",
    "    for left_key, company_data, suffix in merge_targets:\n",
    "        company_col = find_company_column(company_data)\n",
    "        final_df[left_key] = final_df[left_key].str.strip().str.lower()\n",
    "        company_data[company_col] = company_data[company_col].str.strip().str.lower()\n",
    "        company_data = company_data.rename(columns={\n",
    "            col: f\"{col}{suffix}\" for col in company_data.columns if col != company_col\n",
    "        })\n",
    "        final_df = final_df.merge(\n",
    "            company_data,\n",
    "            how='left',\n",
    "            left_on=left_key,\n",
    "            right_on=company_col,\n",
    "            suffixes=('', suffix)\n",
    "        )\n",
    "        final_df = final_df.drop(columns=[company_col], errors='ignore')\n",
    "        final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "\n",
    "    # Prepare final features\n",
    "    with open('selected_features.pkl', 'rb') as f:\n",
    "        selected_features = pickle.load(f)\n",
    "    \n",
    "    final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "    final_df = final_df.reindex(columns=selected_features, fill_value=np.nan)\n",
    "    final_df = final_df.fillna(value=modes)\n",
    "\n",
    "    # Transform and predict\n",
    "    X_new_scaled = final_scaler.transform(final_df)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    with open('best_rf_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = accuracy_score(actual_deal_size, predictions)\n",
    "    print(f\"Prediction Accuracy vs Random: {accuracy:.4f}\")\n",
    "    \n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    messagebox.showinfo(\"Prediction Results\", \n",
    "                       f\"Accuracy against random data: {accuracy:.4f}\\n\"\n",
    "                       f\"Predictions saved to {output_path}\")\n",
    "    root.destroy()\n",
    "\n",
    "    # Save results\n",
    "    output_df = acquisitions_new.copy()\n",
    "    output_df['Predicted_Deal_Size'] = predictions\n",
    "    output_df['Random_Comparison'] = actual_deal_size\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import filedialog, messagebox\n",
    "\n",
    "# def browse_file(entry_widget):\n",
    "#     directory = filedialog.askopenfilename()\n",
    "#     if directory:\n",
    "#         entry_widget.delete(0, tk.END)\n",
    "#         entry_widget.insert(0, directory)\n",
    "\n",
    "# def predict_data():\n",
    "#     # Get input values from entries\n",
    "#     paths = [\n",
    "#         entry1.get(),\n",
    "#         entry2.get(),\n",
    "#         entry3.get()\n",
    "#     ]\n",
    "    \n",
    "#     # Validate paths\n",
    "#     for path in paths:\n",
    "#         if not path.strip():\n",
    "#             messagebox.showerror(\"Error\", \"All paths must be selected!\")\n",
    "#             return\n",
    "    \n",
    "#     # Call your prediction function\n",
    "#     try:\n",
    "#         predict_new_data(*paths, \"new_predictions.csv\")\n",
    "#         messagebox.showinfo(\"Success\", \"Prediction completed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# # Create main window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Predictor\")\n",
    "# root.geometry(\"1200x800\")\n",
    "\n",
    "# # Function to create consistent input fields with browse buttons\n",
    "# def create_file_input(row, label_text):\n",
    "#     # Label\n",
    "#     tk.Label(root, text=label_text).grid(row=row, column=0, padx=10, pady=5, sticky=tk.W)\n",
    "    \n",
    "#     # Entry field\n",
    "#     entry = tk.Entry(root, width=80)\n",
    "#     entry.grid(row=row, column=1, padx=10, pady=5)\n",
    "    \n",
    "#     # Browse button\n",
    "#     browse_btn = tk.Button(root, text=\"Browse\", \n",
    "#                           command=lambda: browse_file(entry))\n",
    "#     browse_btn.grid(row=row, column=2, padx=10, pady=5)\n",
    "    \n",
    "#     return entry\n",
    "\n",
    "# # Create input fields with browse buttons\n",
    "# entry1 = create_file_input(0, \"Acquiring Path:\")\n",
    "# entry2 = create_file_input(1, \"Acquired Path:\")\n",
    "# entry3 = create_file_input(2, \"Acquisitions Path:\")\n",
    "\n",
    "# # Prediction button\n",
    "# predict_button = tk.Button(root, text=\"Predict\", command=predict_data,\n",
    "#                           bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "# predict_button.grid(row=3, column=0, columnspan=3, pady=20, ipadx=10, ipady=5)\n",
    "\n",
    "# # Configure grid layout\n",
    "# root.grid_columnconfigure(1, weight=1)  # Make entry fields expandable\n",
    "\n",
    "# # Start the GUI\n",
    "# root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
