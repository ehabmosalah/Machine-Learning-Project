# -*- coding: utf-8 -*-
"""mlpclass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePdnOTlDlQRobnrmvJD_COMdbiNe8Gcp
"""



!pip install pycountry

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pycountry
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.svm import SVR
from sklearn.ensemble import VotingRegressor
from sklearn.svm import SVC

data =pd.read_csv('Acquiring Tech Companies.csv')

data.isna().sum()

data

data=data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'],axis=1)

data

data.dtypes

data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)
data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)

for col in data.columns:
    if data[col].dtype == 'object':
        try:
            data[col] = data[col].astype(int)
        except:
            pass  # ignore if it fails, i.e., for non-numeric text

data.dtypes

# Replace "Not yet" with NaN first
data['IPO'] = data['IPO'].replace("Not yet", np.nan)

# Create a new binary feature
data['Is_Public'] = data['IPO'].notna().astype(int)

data.drop_duplicates(inplace=True)

data.count()

from sklearn.preprocessing import MultiLabelBinarizer

# Step 1: Split the comma-separated values into lists
data['Market Categories'] = data['Market Categories'].fillna('')  # Handle NaN
data['Market Categories List'] = data['Market Categories'].apply(lambda x: [i.strip() for i in x.split(',') if i.strip()])

# Step 2: Use MultiLabelBinarizer to create one-hot columns
mlb = MultiLabelBinarizer()
category_dummies = pd.DataFrame(mlb.fit_transform(data['Market Categories List']),
                                columns=mlb.classes_,
                                index=data.index)

# Step 3: Concatenate the result with the original dataframe (or drop the original column if you want)
data = pd.concat([data, category_dummies], axis=1)

# Optional: Drop the old columns if not needed
data.drop(columns=['Market Categories', 'Market Categories List'], inplace=True)





data.columns

numerical_data = data.select_dtypes(include=[np.number])

# Select object (categorical) columns
categorical_data = data.select_dtypes(include=['object'])

# Display the first few rows of each
print("Numerical Data:")
print(numerical_data.head())

print("\nCategorical Data:")
print(categorical_data.head())



correlation_matrix = numerical_data.corr()

# Set a threshold for correlation (e.g., 0.85)
threshold = 0.85

# Create a set to store features to drop
to_drop = set()

# Loop through the upper triangle of the correlation matrix
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:  # Check if correlation is above threshold
            colname = correlation_matrix.columns[i]
            to_drop.add(colname)  # Mark column for removal

# Drop the columns from the dataset
data = data.drop(columns=to_drop)

to_drop

null_indexes = data[data['State / Region (HQ)'].isnull()].index
print(null_indexes)

data['State / Region (HQ)'][21]='Finnmark'

data['State / Region (HQ)'][26]='Gyeonggi-do'

data['State / Region (HQ)'][28]='Baden-WÃ¼rttemberg'

null_indexes = data[data['City (HQ)'].isnull()].index
print(null_indexes)

data['City (HQ)'][28]='Walldorf'

data['City (HQ)'][34]='West Berkshire'

data.columns

category_columns = ['Advertising Platforms', 'All Markets', 'All Students', 'Big Data',
       'Blogging Platforms', 'Cloud Computing', 'Communications Hardware',
       'Computers', 'Consumer Goods', 'Creative', 'Curated Web', 'E-Commerce',
       'Electronics', 'Email', 'Enterprise Software', 'Hardware',
       'Hardware + Software', 'Information Technology', 'Messaging', 'Mobile',
       'Networking', 'Photography', 'RIM', 'Search', 'Security',
       'Semiconductors', 'Social Bookmarking', 'Social Media',
       'Social Recruiting', 'Software', 'Storage', 'Telecommunications',
       'Video Games', 'Web Hosting']
# Step 2: Find top 15 most frequent (most used) category columns
top_15 = data[category_columns].sum().sort_values(ascending=False).head(20).index.tolist()

# Step 3: Keep top 15 and compute 'Other' column
df_top = data[top_15].copy()

# Create 'Other' column from all the rest
other_columns = list(set(category_columns) - set(top_15))
df_top['Other'] = data[other_columns].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)

# Step 4: Drop the original one-hot category columns
data.drop(columns=category_columns, inplace=True)

# Step 5: Add the reduced one-hot set back
data = pd.concat([data, df_top], axis=1)

data

data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')

# Then, calculate age (assuming current year is 2025)
data['age'] = 2025 - data['Year Founded']
data.drop(columns=['Year Founded'], inplace=True)

data['IPO'] = pd.to_numeric(data['IPO'], errors='coerce')

# Then, calculate age (assuming current year is 2025)
data['age IPO'] = 2025 - data['IPO']
data.drop(columns=['IPO'], inplace=True)

data['age IPO'].replace(np.nan, -1, inplace=True)

data.isna().sum()



data.drop(columns=['Address (HQ)'], inplace=True)



data['Number of Employees (year of last update)'].fillna(
    data['Number of Employees (year of last update)'].mode()[0], inplace=True
)

data.dtypes

mean_without_zeros = data.loc[data['Number of Employees'] != 0, 'Number of Employees'].mean()

# Step 2: Replace 0s with that mean
data['Number of Employees'] = data['Number of Employees'].replace(0, mean_without_zeros)



# for col in data.columns:
#     if data[col].dtype == 'object':  # or you can use: if data[col].dtype == 'O'
#         le = LabelEncoder()
#         data[col] = data[col].astype(str)  # ensure all values are strings
#         data[col] = le.fit_transform(data[col])

data

from collections import Counter

# Step 1: Combine all board members into a single list
all_members = []

for cell in data['Board Members'].dropna():
    members = [name.strip() for name in cell.split(',')]
    all_members.extend(members)

# Step 2: Count occurrences
member_counts = Counter(all_members)

# Step 3: Convert to DataFrame (optional, for easier viewing)
member_counts_df = pd.DataFrame(member_counts.items(), columns=['Name', 'Count']).sort_values(by='Count', ascending=True)

print(member_counts_df.head(10))
member_counts_df['Count'].value_counts()

data.drop(columns=['Board Members'], inplace=True)

data.isna().sum()

from collections import Counter

# Step 1: Combine all board members into a single list
founders = []

for cell in data['Founders'].dropna():
    members = [name.strip() for name in cell.split(',')]
    all_members.extend(members)

# Step 2: Count occurrences
founder_counts = Counter(all_members)

# Step 3: Convert to DataFrame (optional, for easier viewing)
founder_counts_df2 = pd.DataFrame(founder_counts.items(), columns=['Name', 'Count']).sort_values(by='Count', ascending=True)

print(founder_counts_df2.head(10))
member_counts_df['Count'].value_counts()

data.drop(columns=['Founders'], inplace=True)

data

data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')

data['Text_Combined']

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)  # remove punctuation/numbers
    text = re.sub(r'\s+', ' ', text).strip()  # remove extra whitespace
    return text

data['Text_Clean'] = data['Text_Combined'].apply(clean_text)

data['Text_Clean']

tfidf = TfidfVectorizer(max_features=150)
tfidf_matrix = tfidf.fit_transform(data['Text_Clean'])

tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())
data = pd.concat([data.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)



data.shape

data.drop(columns=['Tagline','Description'], inplace=True)

data.drop(columns=['Text_Clean'], inplace=True)





import re
acquisition_df = pd.read_csv("Acquisitions.csv")
acquired_df = pd.read_csv(r"Acquired Tech Companies.csv")


#acquired_df.head()
acquired_df.columns
print(acquired_df.isnull().sum())

print(acquired_df.duplicated().sum())
#print("status column unique values",acquisition_df["Status"].unique())
#print("Terms column unique values",acquisition_df["Terms"].unique())

acquired_df.info()
# task1 from the address get the country then remove the address




def extract_year(text):
    if not isinstance(text, str) or not text.strip():
        return None

    # Look for 'founded' followed by anything then a 4-digit year
    match = re.search(r'\bfounded\b.*?(\d{4})', text, re.IGNORECASE)
    if match:
        return int(match.group(1))

    return None

print(acquired_df['Year Founded'].isnull().sum())
# Apply the function to 'Tagline' and 'Description' where 'Year Founded' is null
acquired_df['Year Founded'] = acquired_df.apply(
    lambda row: extract_year(row['Tagline']) if pd.isnull(row['Year Founded']) else row['Year Founded'], axis=1
)

# If 'Year Founded' is still null, try using the Description column
acquired_df['Year Founded'] = acquired_df.apply(
    lambda row: extract_year(row['Description']) if pd.isnull(row['Year Founded']) else row['Year Founded'], axis=1
)
print(acquired_df['Year Founded'].isnull().sum())

'''acquired_df['Year Founded'].fillna(acquired_df['Year Founded'].median(), inplace=True)
print(acquired_df['Year Founded'].isnull().sum())'''


# --- Step 4.5: Fill missing 'Market Categories' from 'Tagline' ---

# Fill NaN values with 'Unknown'
acquired_df['Market Categories'] = acquired_df['Market Categories'].fillna('Unknown')

# Define keyword-based categories
category_keywords = {
    "Artificial Intelligence": ["ai", "machine learning", "deep learning", "neural network"],
    "Mobile": ["mobile", "android", "ios", "app store", "smartphone"],
    "E-Commerce": ["ecommerce", "e-commerce", "shopping", "online store"],
    "FinTech": ["finance", "banking", "payments", "fintech", "crypto", "blockchain"],
    "Healthcare": ["health", "medical", "hospital", "doctor", "pharma"],
    "Social Media": ["social network", "community", "messaging", "chat"],
    "Gaming": ["game", "gaming", "video game", "esports"],
    "Cloud": ["cloud", "saas", "paas", "infrastructure"],
    "EdTech": ["education", "learning", "students", "teaching", "school"],
    "Data Analytics": ["analytics", "data science", "big data", "insights"]
}

# Optional: preview unique existing categories
split_categories = acquired_df['Market Categories'].apply(lambda x: [i.strip() for i in x.split(',') if i.strip()])
all_categories = sorted(set(cat for sublist in split_categories for cat in sublist))
print("Sample existing categories:", all_categories[:20])
print("Total unique categories:", len(all_categories))

# Function to guess category from tagline

'''def guess_category_from_tagline(tagline):
    tagline = str(tagline).lower()
    matched = [cat for cat, keywords in category_keywords.items()
               if any(keyword in tagline for keyword in keywords)]
    return ', '.join(matched) if matched else "Unknown"'''


def guess_category_from_tagline(tagline):
    tagline = str(tagline).lower()
    matched = [cat for cat, keywords in category_keywords.items()
               if any(keyword in tagline for keyword in keywords)]

    # Ensure at least 2 values are returned
    if len(matched) == 0:
        matched = ["Software", "Advertising"]
    elif len(matched) == 1:
        matched.append("Software")

    return ', '.join(matched)
# Fill only where category is unknown
acquired_df['Market Categories'] = acquired_df.apply(
    lambda row: guess_category_from_tagline(row['Tagline']) if row['Market Categories'] in ["Unknown", "nan", "", None] else row['Market Categories'],
    axis=1
)

# Define the mapping of specific categories to generalized categories
category_mapping = {
    'Software': 'Technology & Software',
    'Advertising': 'Advertising & Marketing',
    'E-Commerce': 'E-Commerce & Online Services',
    'Mobile': 'Mobile & Consumer Electronics',
    'Games': 'Games & Entertainment',
    'Social Media': 'Social Networking & Communication',
    'Cloud': 'Technology & Software',
    'Finance': 'Finance & Payments',
    'Healthcare': 'Healthcare & Wellness',
    'Semiconductors': 'Technology Hardware',
    'Data Analytics': 'Analytics & Data Science',
    'Search': 'Advertising & Marketing',
    'Video': 'Games & Entertainment',
    'Networking': 'Telecom & Networks',
    'Messaging': 'Social Networking & Communication',
    'Education': 'Education & Learning',
    'News': 'Media & News',
    'Photo Sharing': 'Digital Media & Content',
    'Mobile Payments': 'Finance & Payments',
    'Robotics': 'Games & Entertainment',
    'Music': 'Games & Entertainment',
    'Photo Editing': 'Digital Media & Content',
    'Online Rental': 'E-Commerce & Online Services',
    'Location Based Services': 'Telecom & Networks',
    'Enterprise Software': 'Technology & Software',
    'Video Streaming': 'Games & Entertainment',
    'PaaS': 'Technology & Software',
    'SaaS': 'Technology & Software',
    'Health and Wellness': 'Healthcare & Wellness',
    'Web Hosting': 'Technology & Software',
    'Internet of Things': 'IoT (Internet of Things)',
    'Cloud Security': 'Technology & Software',
    'Virtual Currency': 'Finance & Payments',
    'Search Marketing': 'Advertising & Marketing',
    'Mobile Social': 'Social Networking & Communication',
    'Retail': 'Retail & Fashion',
    'Consulting': 'Others & Miscellaneous',
    'Aerospace': 'Others & Miscellaneous',
    'Food Delivery': 'Consumer Goods & Services',
    'Fashion': 'Retail & Fashion',
    'Wine And Spirits': 'Consumer Goods & Services',
    'Streaming': 'Games & Entertainment',
    'Task Management': 'Others & Miscellaneous',
    'Video Chat': 'Social Networking & Communication',
    'Personalization': 'Advertising & Marketing',
    'Shopping': 'E-Commerce & Online Services',
    'Local': 'E-Commerce & Online Services',
    'News': 'Media & News',
    'Fraud Detection': 'Advertising & Marketing',
    'Image Recognition': 'Technology Hardware',
    'Virtualization': 'Games & Entertainment',
    'Analytics': 'Analytics & Data Science',
    'Video on Demand': 'Games & Entertainment',
    'Mobile Payments': 'Finance & Payments',
    'Marketing Automation': 'Advertising & Marketing',
    'Consumer Electronics': 'Mobile & Consumer Electronics',
    'Video Games': 'Games & Entertainment',
    'Public Relations': 'Advertising & Marketing'
}

# Add any other specific categories to the mapping above

# Function to map specific categories to the generalized ones
def map_to_generalized_category(row):
    # Split the current market categories and map each category
    categories = row.split(',')  # Split by comma
    generalized_categories = []

    for category in categories:
        category = category.strip()  # Remove any extra spaces
        if category in category_mapping:
            generalized_categories.append(category_mapping[category])
        else:
            generalized_categories.append('Others & Miscellaneous')  # In case of unknown categories

    return ', '.join(set(generalized_categories))  # Return unique generalized categories as a string

# Apply the mapping function to the 'Market Categories' column
acquired_df['Generalized Market Categories'] = acquired_df['Market Categories'].apply(map_to_generalized_category)

# Print the full output without truncation
print(acquired_df[['Market Categories', 'Generalized Market Categories']].to_string(index=False))

acquired_df.loc[acquired_df['Company']=="Emagic"]
acquired_df.info()

print(acquired_df.isnull().sum())

# Prepare country list
countries = [country.name for country in pycountry.countries]
regions = ['California', 'New York', 'Texas', 'Basel', 'Utah', 'ÃŽle-de-France', 'Bavaria', 'Ontario',
           'Switzerland', 'United States', 'France', 'Great Britain',
       'Israel', 'Sweden', 'Canada', 'Germany', 'Japan', 'India',
       'Denmark', 'China', 'Spain', 'Netherlands', 'Finland', 'Australia',
       'Ireland', 'United Stats of AMerica', 'United Arab Emirates'
           'Quebec',]  # extend as needed

# Helper function to find first match
def find_place(text, place_list):
    for place in place_list:
        if re.search(r'\b' + re.escape(place) + r'\b', str(text)):
            return place
    return None

# Loop through rows with nulls
for idx, row in acquired_df[acquired_df['Country (HQ)'].isnull() | acquired_df['State / Region (HQ)'].isnull()].iterrows():
    desc = row['Description']
    if pd.isnull(desc):
        continue
    # Try to extract country and region
    country = find_place(desc, countries)
    region = find_place(desc, regions)
    if pd.isnull(row['Country (HQ)']) and country:
        acquired_df.at[idx, 'Country (HQ)'] = country
    if pd.isnull(row['State / Region (HQ)']) and region:
        acquired_df.at[idx, 'State / Region (HQ)'] = region


columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter','Address (HQ)','API','Description','Tagline','Market Categories']
acquired_df.drop(columns=columns_to_drop, inplace=True)
print(acquired_df.columns)


import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Fill nulls in categorical columns using mode
for col in ['City (HQ)', 'State / Region (HQ)', 'Country (HQ)']:
    mode_value = acquired_df[col].mode()[0]
    acquired_df[col].fillna(mode_value, inplace=True)

# Label Encoding
label_encoders = {}
for col in ['City (HQ)', 'State / Region (HQ)', 'Country (HQ)']:
    le = LabelEncoder()
    acquired_df[col + '_LabelEncoded'] = le.fit_transform(acquired_df[col].astype(str))
    label_encoders[col] = le

# One-hot encoding
df_one_hot = pd.get_dummies(acquired_df[['City (HQ)', 'State / Region (HQ)', 'Country (HQ)']],
                            prefix=['City', 'State', 'Country'])

# Drop the original columns
acquired_df.drop(columns=['City (HQ)', 'State / Region (HQ)', 'Country (HQ)'], inplace=True)

# Combine everything
df_final = pd.concat([acquired_df, df_one_hot], axis=1)

# View the result
print(df_final.head())
from sklearn.preprocessing import MultiLabelBinarizer
import pandas as pd

# Step 1: Convert the 'Generalized Market Categories' column to a list of categories
acquired_df['MultiCategoryList'] = acquired_df['Generalized Market Categories'].apply(
    lambda x: [cat.strip() for cat in x.split(',')] if pd.notnull(x) else []
)

# Step 2: Initialize the MultiLabelBinarizer
mlb = MultiLabelBinarizer()

# Step 3: Transform the list column to one-hot encoded format
multi_hot_encoded = mlb.fit_transform(acquired_df['MultiCategoryList'])

# Step 4: Convert to DataFrame with proper column names
df_multi_hot = pd.DataFrame(multi_hot_encoded, columns=mlb.classes_, index=acquired_df.index)

# Step 5: Drop the helper column
acquired_df.drop(columns=['MultiCategoryList'], inplace=True)

# Step 6: Merge back the encoded columns
acquired_df = pd.concat([acquired_df, df_multi_hot], axis=1)

# Final result
print(acquired_df.head())


acquired_df.columns

acquired_df.drop(columns=['Generalized Market Categories'], inplace=True)

acquired_df.columns

acquired_df.drop(columns=['Year Founded'], inplace=True)

acquired_df['Acquired by'].fillna('Salesforce', inplace=True)

acquired_df.isna().sum()

# Importing Libraries

# Import and Overview the data

df = pd.read_csv('Acquisitions.csv')
df.head()
df.info()
# Check for missing values
print(df.isnull().sum())

value_counts_1 = df['Status'].value_counts()
value_counts_2 = df['Terms'].value_counts()

# Print unique values and their frequencies
print(value_counts_1)
print('------------------')
print(value_counts_2)


### Filling Null Values
# Get mode of the column
mode_value = df['Status'].mode()[0]

# Fill NaN values with the mode
df['Status'].fillna(mode_value, inplace=True)
### One-Hot Encoding
 #One-hot encode the 'Status' column
df = pd.get_dummies(df, columns=['Status'], drop_first=False)
# One-hot encode the 'Terms' column
df = pd.get_dummies(df, columns=['Terms'], drop_first=False)
df.head()
### Drop unnecessary columns
# Apply transformation to remove 'Cash,Stock' from the 'Terms' column
combined_mask = df['Terms_Cash, Stock'] == True
df.loc[combined_mask, 'Terms_Cash'] = True
df.loc[combined_mask, 'Terms_Stock'] = True

# Drop combined column
df = df.drop('Terms_Cash, Stock', axis=1)
#Drop unuseful columns that are not needed for the analysis
df.drop(columns=["Acquisition Profile", "News", "News Link"], inplace=True)
# Recheck after conversion
print("\nðŸ”¹ Data Types After Converting 'Price':")
print(df.dtypes)
## Extract Date Features
# Convert 'Deal announced on' to datetime
df['Deal_date'] = pd.to_datetime(df['Deal announced on'], dayfirst=True, errors='coerce')


# Extract day, month, and day of week
df['Deal_day'] = df['Deal_date'].dt.day
df['Deal_month'] = df['Deal_date'].dt.month
df['Deal_dayofweek'] = df['Deal_date'].dt.dayofweek  # Monday=0, Sunday=6

# Drop combined column
df = df.drop('Deal announced on', axis=1)
df = df.drop('Deal_date', axis=1)
df.info()
df.head()

null_indices = df[df.isna().any(axis=1)].index

# Display the indices with null values
print(null_indices)

for column in df.columns:
    mode_value = df[column].mode()[0]  # Get the mode value of the column
    df[column].fillna(mode_value, inplace=True)

# Check the result
print(df.isna().sum())

def find_company_column(df):
    for col in df.columns:
        if "company" in col.lower():
            return col
    raise ValueError("No company name column found.")

# Start with acquisition data
final_df = df.copy()

# Mapping for left_on column and its corresponding dataset
merge_targets = [
    ('Acquired Company', acquired_df, '_Acquired'),
    ('Acquiring Company', data, '_Acquiring')
]

# Perform merges in loop
for left_key, company_data, suffix in merge_targets:
    company_col = find_company_column(company_data)

    # Strip and lower case the company names for matching
    final_df[left_key] = final_df[left_key].str.strip().str.lower()
    company_data[company_col] = company_data[company_col].str.strip().str.lower()

    # Merge
    final_df = final_df.merge(
        company_data,
        how='left',
        left_on=left_key,
        right_on=company_col,
        suffixes=('', suffix)
    )

    # Drop the extra company column if you want
    final_df.drop(columns=[company_col], inplace=True, errors='ignore')



# Done!
print(final_df.head())

final_df.shape

null_indices = final_df[final_df.isna().any(axis=1)].index

# Print them
print(null_indices)
final_df.dropna(axis=0, inplace=True)

final_df.shape

first_column = acquired_df.columns[0]

# Define the value you're searching for
search_value = "hotjobs"  # or any value you want to look for

# Find rows where the first column matches the value
matching_rows = acquired_df[acquired_df[first_column].str.strip().str.lower() == search_value.strip().lower()]
print(matching_rows)



for x in final_df.isna().sum():
    print(x)

from sklearn.preprocessing import LabelEncoder

# Make a copy so you donâ€™t overwrite your original
encoded_df = final_df.copy()

# Loop over all columns
for col in encoded_df.columns:
    if encoded_df[col].dtype in ['object', 'bool']:  # Include strings and booleans
        le = LabelEncoder()
        encoded_df[col] = encoded_df[col].astype(str)  # Convert all values to string
        encoded_df[col] = le.fit_transform(encoded_df[col])

encoded_df



for col in encoded_df.columns:
    if (encoded_df[col].nunique() <= 1) or (encoded_df[col].eq(0).all()):
        encoded_df.drop(columns=col, inplace=True)

encoded_df.shape

encoded_df['Deal size class']



for x in df.isna().sum():
    print(x)

for x in encoded_df.columns:
    print(x)

correlation_matrix = encoded_df.corr()

# Get correlation of each feature with 'price'
price_correlation = correlation_matrix['Deal size class']

# Set the correlation threshold (e.g., absolute value of correlation > 0.1)
threshold = 0.15

# Identify features with correlation less than the threshold (in absolute value)
low_correlation_features = price_correlation[price_correlation.abs() < threshold].index

# Drop features with low correlation
X_train_filtered = encoded_df.drop(columns=low_correlation_features)

# Print the filtered dataset
print("Features dropped due to low correlation with 'Deal size class':")
print(low_correlation_features)







X = X_train_filtered.drop(columns=['Deal size class'])
y = X_train_filtered['Deal size class']
#y_log = np.log1p(y)  # log1p(x) = log(x + 1), handles zeros safely
column=X.columns

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# from sklearn.feature_selection import SelectKBest, f_regression

# # Select top 5 features using f_regression
# x_data_kbest_train = SelectKBest(f_regression, k=15).fit_transform(X, y)
# print("train", x_data_kbest_train.shape)

X_train, X_test, y_train_log, y_test_log = train_test_split(X_scaled, y, test_size=0.2, random_state=42)



from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # Retain 95% of variance

pca.fit(X_train)

# Transform training and test data
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

from sklearn.model_selection import cross_val_score, train_test_split

y_train_log

C_values = [0.1, 1, 10]
for c in C_values:
    model = SVC(C=c, kernel='rbf', random_state=42)
    scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"C={c}, Accuracy={scores.mean():.3f}")

from sklearn.metrics import accuracy_score

C_values = [0.1, 1, 10]
for c in C_values:
    model = SVC(C=c, kernel='rbf', random_state=42)

    # Cross-validation on training data
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"C={c}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"C={c}, Test Accuracy={test_acc:.3f}\n")

C_fixed = 1
kernel_types = ['linear', 'rbf', 'poly', 'sigmoid']

for kernel in kernel_types:
    model = SVC(C=C_fixed, kernel=kernel, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"Kernel='{kernel}', CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Predict on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"Kernel='{kernel}', Test Accuracy={test_acc:.3f}\n")

C_fixed = 1
kernel_fixed = 'sigmoid'
gamma_values = ['scale', 'auto', 0.01, 0.1, 1]

for gamma in gamma_values:
    model = SVC(C=C_fixed, kernel=kernel_fixed, gamma=gamma, random_state=42)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"gamma={gamma}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"gamma={gamma}, Test Accuracy={test_acc:.3f}\n")

C_values = [0.1, 1, 10]
for c in C_values:
    model = SVC(C=c, kernel='rbf', random_state=42)

    # Cross-validation on training data
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"C={c}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"C={c}, Test Accuracy={test_acc:.3f}\n")

C_fixed = 1
kernel_types = ['linear', 'rbf', 'poly', 'sigmoid']

for kernel in kernel_types:
    model = SVC(C=C_fixed, kernel=kernel, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"Kernel='{kernel}', CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Predict on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"Kernel='{kernel}', Test Accuracy={test_acc:.3f}\n")

C_fixed = 1
kernel_fixed = 'sigmoid'
gamma_values = ['scale', 'auto', 0.01, 0.1, 1,10]

for gamma in gamma_values:
    model = SVC(C=C_fixed, kernel=kernel_fixed, gamma=gamma, random_state=42)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"gamma={gamma}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"gamma={gamma}, Test Accuracy={test_acc:.3f}\n")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

# Example values to test
n_estimators_list = [50, 100, 200,300]

# Fixed parameters
max_depth_fixed = 10

for n in n_estimators_list:
    model = RandomForestClassifier(n_estimators=n, max_depth=max_depth_fixed, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

n_estimators_list =  200

# Fixed parameters
max_depth_fixed = [10, 20, 50,100]

for n in max_depth_fixed:
    model = RandomForestClassifier(n_estimators=n_estimators_list, max_depth=n, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

n_estimators_list =  200

# Fixed parameters
max_depth_fixed = 10
min=[10,8,20,50]
for n in min:
    model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=n,random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

# Example values to test
n_estimators_list = [50, 100, 200,300]

# Fixed parameters
max_depth_fixed = 10

for n in n_estimators_list:
    model = RandomForestClassifier(n_estimators=n, max_depth=max_depth_fixed, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

n_estimators_list =  400

# Fixed parameters
max_depth_fixed = [10, 20, 50,100]

for n in max_depth_fixed:
    model = RandomForestClassifier(n_estimators=n_estimators_list, max_depth=n, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

n_estimators_list =  200

# Fixed parameters
max_depth_fixed = 1
min=[10,20,50]
for n in min:
    model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=n,random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"n_estimators={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_estimators={n}, Test Accuracy={test_acc:.3f}\n")

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Define the hyperparameter options
max_depth_values = [3, 6, 9]  # Choices for max_depth
learning_rate_values = [0.01, 0.1, 0.3]  # Choices for learning_rate

# Fixed hyperparameters
n_estimators = 100
objective = 'multi:softmax'  # Multi-class classification
num_class = len(set(y))  # Number of classes in the target variable

# Loop through different choices of max_depth
for max_depth in max_depth_values:
    print(f"Training with max_depth = {max_depth}")

    # Loop through different choices of learning_rate
    for learning_rate in learning_rate_values:
        print(f"  Training with learning_rate = {learning_rate}")

        # Initialize the XGBoost classifier with the current hyperparameters
        model = xgb.XGBClassifier(
            max_depth=max_depth,
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            objective=objective,
            num_class=num_class,
            random_state=42
        )

        # Train the model
        model.fit(X_train, y_train_log)

        # Predict on the test set
        y_pred = model.predict(X_test)

        # Calculate the accuracy
        accuracy = accuracy_score(y_test_log, y_pred)

        print(f"    Accuracy for max_depth={max_depth}, learning_rate={learning_rate}: {accuracy:.4f}")

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Fixed parameters
n_estimators_list = 200
max_depth_fixed = [3, 6, 10, 20]

# Loop through different values of max_depth
for n in max_depth_fixed:
    model = GradientBoostingClassifier(n_estimators=n_estimators_list, max_depth=n, learning_rate=0.2,random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"max_depth={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"max_depth={n}, Test Accuracy={test_acc:.3f}\n")

# Fixed parameters
n_estimators_list = [100,50,200]

# Loop through different values of max_depth
for n in n_estimators_list:
    model = GradientBoostingClassifier(n_estimators=n, max_depth=3, learning_rate=0.2,random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"max_depth={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"max_depth={n}, Test Accuracy={test_acc:.3f}\n")

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# Fixed parameters
n_estimators_list = 200
max_depth_fixed = [3, 6, 10, 20]

# Loop through different values of max_depth
for n in max_depth_fixed:
    model = GradientBoostingClassifier(n_estimators=n_estimators_list, max_depth=n, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"max_depth={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"max_depth={n}, Test Accuracy={test_acc:.3f}\n")

# Fixed parameters
n_estimators_list = [100,50,200,300]
max_depth_fixed = 6

# Loop through different values of max_depth
for n in n_estimators_list:
    model = GradientBoostingClassifier(n_estimators=n, max_depth=3, random_state=42)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"max_depth={n}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"max_depth={n}, Test Accuracy={test_acc:.3f}\n")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score

n_neighbors_list = [3, 5, 7, 9, 11]

# Loop over different values of n_neighbors
for k in n_neighbors_list:
    model = KNeighborsClassifier(n_neighbors=k)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"n_neighbors={k}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train_pca, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_neighbors={k}, Test Accuracy={test_acc:.3f}\n")

n_neighbors_list = [3, 5, 7, 9, 11]

# Loop over different values of n_neighbors
for k in n_neighbors_list:
    model = KNeighborsClassifier(n_neighbors=k)

    # Cross-validation on training set
    cv_scores = cross_val_score(model, X_train, y_train_log, cv=5)
    print(f"n_neighbors={k}, CV Accuracy={cv_scores.mean():.3f}")

    # Train on full training set
    model.fit(X_train, y_train_log)

    # Evaluate on test set
    y_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"n_neighbors={k}, Test Accuracy={test_acc:.3f}\n")

# Fixed hyperparameters
n_neighbors_fixed = 11
weights_fixed = 'uniform'

# Algorithms to try
algorithm_list = ['auto', 'ball_tree', 'kd_tree', 'brute']

# Loop over algorithms
for algo in algorithm_list:
    model = KNeighborsClassifier(n_neighbors=n_neighbors_fixed, weights=weights_fixed, algorithm=algo)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_pca, y_train_log, cv=5)
    print(f"algorithm={algo}, CV Accuracy={cv_scores.mean():.3f}")

    # Train and test
    model.fit(X_train_pca, y_train_log)
    y_pred = model.predict(X_test_pca)
    test_acc = accuracy_score(y_test_log, y_pred)
    print(f"algorithm={algo}, Test Accuracy={test_acc:.3f}\n")

from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Split data

# Define models
svc_model = SVC(kernel='sigmoid', C=1, gamma='auto', probability=True)
#rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.2, random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=11, algorithm='ball_tree', weights='uniform')
xgb_mv = xgb.XGBClassifier(
            max_depth=3,
            learning_rate=0.1,
            n_estimators=100,
            objective= 'multi:softmax',

            random_state=42
        )

# Voting classifier
voting_clf = VotingClassifier(
    estimators=[
        ('svc', svc_model),
        ('xgb', xgb_mv),
        ('gb', gb_model),
        ('knn', knn_model),

    ],
    voting='soft'
)

# Cross-validation accuracy (on training set)
cv_scores = cross_val_score(voting_clf, X_train_pca, y_train_log, cv=5)
print(f"Voting Classifier CV Accuracy: {cv_scores.mean():.3f}")

# Train on full training set
voting_clf.fit(X_train_pca, y_train_log)

# Test accuracy
y_pred = voting_clf.predict(X_test_pca)
test_accuracy = accuracy_score(y_test_log, y_pred)
print(f"Voting Classifier Test Accuracy: {test_accuracy:.3f}")

from sklearn.ensemble import StackingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb

# Define base models
svc_model = SVC(kernel='sigmoid', C=1, gamma='auto', probability=True)
gb_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)
knn_model = KNeighborsClassifier(n_neighbors=11, algorithm='ball_tree', weights='uniform')
xgb_mv = xgb.XGBClassifier(
    max_depth=3,
    learning_rate=0.1,
    n_estimators=100,
    objective='multi:softmax',
    random_state=42,
    use_label_encoder=False,
    eval_metric='mlogloss'
)

# Meta-model (final estimator)
meta_model = LogisticRegression(max_iter=700)

# Stacking classifier
stacking_clf = StackingClassifier(
    estimators=[
        ('svc', svc_model),
        ('xgb', xgb_mv),
        ('gb', gb_model),
        ('knn', knn_model),
    ],
    final_estimator=meta_model,
    cv=5,
    passthrough=False,  # Set to True if you want to include original features for meta-model
    n_jobs=-1
)

# Cross-validation on training set
cv_scores = cross_val_score(stacking_clf, X_train, y_train_log, cv=5)
print(f"Stacking Classifier CV Accuracy: {cv_scores.mean():.3f}")

# Train on full training set
stacking_clf.fit(X_train, y_train_log)

# Evaluate on test set
y_pred = stacking_clf.predict(X_test)
test_accuracy = accuracy_score(y_test_log, y_pred)
print(f"Stacking Classifier Test Accuracy: {test_accuracy:.3f}")

# Initialize the SVC model
svc_model = SVC(kernel='sigmoid', C=1, gamma='auto')

# Cross-validation on training set
cv_scores_svc = cross_val_score(svc_model, X_train, y_train_log, cv=5)
print(f"SVC Model CV Accuracy: {cv_scores_svc.mean():.3f}")

# Train on full training set
svc_model.fit(X_train, y_train_log)

# Evaluate on test set
y_pred_svc = svc_model.predict(X_test)
test_acc_svc = accuracy_score(y_test_log, y_pred_svc)
print(f"SVC Model Test Accuracy: {test_acc_svc:.3f}\n")

rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10, random_state=42)

# Cross-validation on training set
cv_scores_rf = cross_val_score(rf_model, X_train_pca, y_train_log, cv=5)
print(f"Random Forest Model CV Accuracy: {cv_scores_rf.mean():.3f}")

# Train on full training set
rf_model.fit(X_train_pca, y_train_log)

# Evaluate on test set
y_pred_rf = rf_model.predict(X_test_pca)
test_acc_rf = accuracy_score(y_test_log, y_pred_rf)
print(f"Random Forest Model Test Accuracy: {test_acc_rf:.3f}\n")

gb_model = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.2, random_state=42)

# Cross-validation on training set
cv_scores_gb = cross_val_score(gb_model, X_train_pca, y_train_log, cv=5)
print(f"Gradient Boosting Model CV Accuracy: {cv_scores_gb.mean():.3f}")

# Train on full training set
gb_model.fit(X_train_pca, y_train_log)

# Evaluate on test set
y_pred_gb = gb_model.predict(X_test_pca)
test_acc_gb = accuracy_score(y_test_log, y_pred_gb)
print(f"Gradient Boosting Model Test Accuracy: {test_acc_gb:.3f}\n")

knn_model = KNeighborsClassifier(n_neighbors=11, algorithm='ball_tree', weights='uniform')

# Cross-validation on training set
cv_scores_knn = cross_val_score(knn_model, X_train_pca, y_train_log, cv=5)
print(f"KNeighbors Model CV Accuracy: {cv_scores_knn.mean():.3f}")

# Train on full training set
knn_model.fit(X_train_pca, y_train_log)

# Evaluate on test set
y_pred_knn = knn_model.predict(X_test_pca)
test_acc_knn = accuracy_score(y_test_log, y_pred_knn)
print(f"KNeighbors Model Test Accuracy: {test_acc_knn:.3f}\n")

# Initialize the XGBoost model
xgb_mv = xgb.XGBClassifier(
    max_depth=3,
    learning_rate=0.2,
    n_estimators=100,
    objective='multi:softmax',
    random_state=42
)

# Cross-validation on training set
cv_scores_xgb = cross_val_score(xgb_mv, X_train, y_train_log, cv=5)
print(f"XGBoost Model CV Accuracy: {cv_scores_xgb.mean():.3f}")

# Train on full training set
xgb_mv.fit(X_train, y_train_log)

# Evaluate on test set
y_pred_xgb = xgb_mv.predict(X_test)
test_acc_xgb = accuracy_score(y_test_log, y_pred_xgb)
print(f"XGBoost Model Test Accuracy: {test_acc_xgb:.3f}\n")

