{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.4.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'correlation_filter.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 261\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mpredict_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquiring.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquired_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquired.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquisitions_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquisitions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_predictions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mpredict_new_data\u001b[0;34m(acquiring_path, acquired_path, acquisitions_path, output_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlb_acquiring.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m     mlb_acquiring \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorrelation_filter.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m     loaded_filter \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'correlation_filter.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import pycountry\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    \"\"\"\n",
    "    Process new data and make predictions using saved models\n",
    "    \n",
    "    Args:\n",
    "        acquiring_path: Path to new acquiring companies CSV\n",
    "        acquired_path: Path to new acquired companies CSV  \n",
    "        acquisitions_path: Path to new acquisitions CSV\n",
    "        output_path: Where to save predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================================\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    # ===================================\n",
    "    \n",
    "    # Load acquiring company transformers\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    \n",
    "    with open(\"correlation_filter.pkl\", \"rb\") as f:\n",
    "        loaded_filter = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    with open(\"category_reducer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_reducer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open('Age_column_acquiring.pkl', 'rb') as f:\n",
    "        age_mode_col =  pickle.load(f)\n",
    "    #\n",
    "    with open(\"ipo_transformer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_ipo_transformer = pickle.load(f)\n",
    "    #\n",
    "    with open(\"employee_cleaner_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_employee_cleaner = pickle.load(f)\n",
    "\n",
    "\n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "        \n",
    "    #\n",
    "    with open(\"tagline_guesser_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_guesser = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"category_generalizer_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_generalizer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"country_region_filler_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_filler = pickle.load(f)\n",
    "    \n",
    "    #\n",
    "    with open(\"categorical_encoder_acquired.pkl\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    # Load acquired company transformers\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    with open('label_encoders_acquired.pkl', 'rb') as f:\n",
    "        label_encoders_acquired = pickle.load(f)\n",
    "        \n",
    "    # Load acquisitions transformers\n",
    "    with open('ohe_acquisitions.pkl', 'rb') as f:\n",
    "        ohe_acquisitions = pickle.load(f)\n",
    "        \n",
    "    # Load final preprocessing objects\n",
    "    with open('final_imputer.pkl', 'rb') as f:\n",
    "        final_imputer = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    with open('final_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # ==============================\n",
    "    # Preprocess New Data (Same as Training)\n",
    "    # ==============================\n",
    "    \n",
    "    # Process each dataset with saved transformers\n",
    "    def process_acquiring_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Apply same cleaning as training\n",
    "        data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'], \n",
    "                 axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        \n",
    "        # Use mean from training (would need to save this)\n",
    "        data['Number of Employees'] = data['Number of Employees'].replace(\n",
    "            0, 450)  # Replace with saved mean from training\n",
    "        \n",
    "        # Handle IPO status\n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data.drop('IPO', axis=1, inplace=True)\n",
    "\n",
    "        # Process Market Categories with saved MLB\n",
    "        data['Market Categories'] = data['Market Categories'].fillna('')\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories'].str.split(',')),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        data = loaded_filter.transform(data)\n",
    "\n",
    "\n",
    "        # Use the loaded reducer on new data\n",
    "        data = loaded_reducer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = age_mode_col.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_ipo_transformer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_employee_cleaner.transform(data)\n",
    "\n",
    "\n",
    "\n",
    "        # Process text with saved TF-IDF\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
    "            return text\n",
    "\n",
    "        data['Text_Combined'] = data['Text_Combined'].apply(clean_text)\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Final cleanup\n",
    "        data = pd.concat([data, category_dummies, tfidf_df], axis=1)\n",
    "        data.drop(['Market Categories', 'Tagline', 'Description', 'Text_Combined','Address (HQ)',\n",
    "                   'Board Members','Founders'], \n",
    "                 axis=1, inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Similar functions for acquired and acquisitions...\n",
    "    # (Implementation would mirror the training preprocessing but using saved transformers)\n",
    "    \n",
    "\n",
    "    def process_acquisitions_new(data):\n",
    "        \"\"\"Process new acquisitions data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Load saved preprocessing objects\n",
    "        with open('custom_acquisitions_encoder.pkl', 'rb') as f:\n",
    "            encoder = pickle.load(f)\n",
    "        with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "            modes = pickle.load(f)\n",
    "        \n",
    "        # Drop columns same as training\n",
    "        data.drop(columns=[\"Acquisition Profile\", \"News\", \"News Link\"], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        # Handle missing values with saved modes\n",
    "        for col, mode_val in modes.items():\n",
    "            if col in data.columns and mode_val is not None:\n",
    "                data[col].fillna(mode_val, inplace=True)\n",
    "        \n",
    "        # Convert date features same as training\n",
    "        if 'Deal announced on' in data.columns:\n",
    "            data['Deal_date'] = pd.to_datetime(data['Deal announced on'], dayfirst=True, errors='coerce')\n",
    "            data['Deal_day'] = data['Deal_date'].dt.day\n",
    "            data['Deal_month'] = data['Deal_date'].dt.month\n",
    "            data['Deal_dayofweek'] = data['Deal_date'].dt.dayofweek\n",
    "            data.drop(['Deal announced on', 'Deal_date'], axis=1, inplace=True)\n",
    "        \n",
    "        # Apply saved one-hot encoding\n",
    "        encoded_status_terms = encoder.transform(data)\n",
    "        data = pd.concat([data, encoded_status_terms], axis=1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process_acquired_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "\n",
    "        data = loaded_guesser.transform(data)\n",
    "\n",
    "        data = loaded_generalizer.transform(data)\n",
    "\n",
    "        data = loaded_filler.transform(data)\n",
    "\n",
    "        data = encoder.transform(data)\n",
    "        \n",
    "        data['Acquired by'].fillna('Salesforce', inplace=True)\n",
    "\n",
    "\n",
    "        columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter','Address (HQ)','API','Description',\n",
    "                           'Tagline','Market Categories','City (HQ)', 'State / Region (HQ)', 'Country (HQ)','Generalized Market Categories','Year Founded']\n",
    "        data.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Preprocess each new dataset\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path)) \n",
    "    acquisitions_new = process_acquisitions_new(pd.read_csv(acquisitions_path))\n",
    "    \n",
    "    # Merge the new data (same as training)\n",
    "    final_new = merge_datasets(\n",
    "        acquiring_new, acquired_new, acquisitions_new, save_artifacts=False\n",
    "    )\n",
    "    \n",
    "    # Handle missing values with saved imputer\n",
    "    final_new_imputed = pd.DataFrame(\n",
    "        final_imputer.transform(final_new),\n",
    "        columns=final_new.columns\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    X_new = final_new_imputed.drop(\n",
    "        ['Deal size class', 'Acquired Company', 'Acquiring Company'], \n",
    "        axis=1, errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Apply same scaling and PCA as training\n",
    "    X_new_scaled = final_scaler.transform(X_new)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Save predictions with original data\n",
    "    final_new['Predicted_Deal_Size'] = predictions\n",
    "    final_new.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def browse_directory(entry_widget):\n",
    "    directory = filedialog.askdirectory()\n",
    "    if directory:\n",
    "        entry_widget.delete(0, tk.END)\n",
    "        entry_widget.insert(0, directory)\n",
    "\n",
    "def predict_data():\n",
    "    # Get input values from entries\n",
    "    paths = [\n",
    "        entry1.get(),\n",
    "        entry2.get(),\n",
    "        entry3.get()\n",
    "    ]\n",
    "    \n",
    "    # Validate paths\n",
    "    for path in paths:\n",
    "        if not path.strip():\n",
    "            messagebox.showerror(\"Error\", \"All paths must be selected!\")\n",
    "            return\n",
    "    \n",
    "    # Call your prediction function\n",
    "    try:\n",
    "        predict_new_data(*paths, \"new_predictions.csv\")\n",
    "        messagebox.showinfo(\"Success\", \"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Predictor\")\n",
    "root.geometry(\"1200x800\")\n",
    "\n",
    "# Function to create consistent input fields with browse buttons\n",
    "def create_file_input(row, label_text):\n",
    "    # Label\n",
    "    tk.Label(root, text=label_text).grid(row=row, column=0, padx=10, pady=5, sticky=tk.W)\n",
    "    \n",
    "    # Entry field\n",
    "    entry = tk.Entry(root, width=80)\n",
    "    entry.grid(row=row, column=1, padx=10, pady=5)\n",
    "    \n",
    "    # Browse button\n",
    "    browse_btn = tk.Button(root, text=\"Browse\", \n",
    "                          command=lambda: browse_directory(entry))\n",
    "    browse_btn.grid(row=row, column=2, padx=10, pady=5)\n",
    "    \n",
    "    return entry\n",
    "\n",
    "# Create input fields with browse buttons\n",
    "entry1 = create_file_input(0, \"Acquiring Path:\")\n",
    "entry2 = create_file_input(1, \"Acquired Path:\")\n",
    "entry3 = create_file_input(2, \"Acquisitions Path:\")\n",
    "\n",
    "# Prediction button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_data,\n",
    "                          bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "predict_button.grid(row=3, column=0, columnspan=3, pady=20, ipadx=10, ipady=5)\n",
    "\n",
    "# Configure grid layout\n",
    "root.grid_columnconfigure(1, weight=1)  # Make entry fields expandable\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
