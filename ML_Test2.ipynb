{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698874ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "from collections import Counter\n",
    "from joblib import dump\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pycountry\n",
    "import re\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================== CLASSES ========================\n",
    "class CorrelationFilter:\n",
    "\n",
    "    def __init__(self, threshold=0.85):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = set()\n",
    "\n",
    "    def fit(self, data):\n",
    "        numerical_data = data.select_dtypes(include=[np.number])\n",
    "        corr_matrix = numerical_data.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    self.to_drop.add(colname)\n",
    "\n",
    "    def transform(self, data):\n",
    "        return data.drop(columns=list(self.to_drop & set(data.columns)), errors='ignore')\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class CategoryReducer:\n",
    "\n",
    "    def __init__(self, category_columns, top_n=15):\n",
    "        self.category_columns = category_columns\n",
    "        self.top_n = top_n\n",
    "        self.top_categories = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.top_categories = data[self.category_columns].sum().sort_values(ascending=False).head(self.top_n).index.tolist()\n",
    "\n",
    "    def transform(self, data):\n",
    "        df_top = data[self.top_categories].copy()\n",
    "        other_columns = list(set(self.category_columns) - set(self.top_categories))\n",
    "        df_top['Other'] = data[other_columns].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "        data = data.drop(columns=self.category_columns)\n",
    "        data = pd.concat([data, df_top], axis=1)\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class AgeTransformer:\n",
    "\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "        self.age_mode = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        data = data.copy()\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        mode_series = data['age'].mode()\n",
    "        if not mode_series.empty:\n",
    "            self.age_mode = mode_series[0]\n",
    "        else:\n",
    "            self.age_mode = 5\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Applies the transformation: computes age and fills missing values.\"\"\"\n",
    "        data = data.copy()\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        data.drop(columns=['Year Founded'], inplace=True)\n",
    "        data['age'].fillna(self.age_mode, inplace=True)\n",
    "        return data\n",
    "class IPOAgeTransformer:\n",
    "\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['IPO'] = pd.to_numeric(df['IPO'], errors='coerce')\n",
    "        df['age IPO'] = self.current_year - df['IPO']\n",
    "        df['age IPO'].replace(np.nan, -1, inplace=True)\n",
    "        if 'IPO' in df.columns:\n",
    "            df.drop(columns=['IPO'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class EmployeeDataCleaner:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.employee_mode = None\n",
    "        self.mean_without_zeros = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.employee_mode = data['Number of Employees (year of last update)'].mode()[0] if not data['Number of Employees (year of last update)'].mode().empty else 0\n",
    "        self.mean_without_zeros = data.loc[data['Number of Employees'] > 0, 'Number of Employees'].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        if 'Number of Employees (year of last update)' in df.columns:\n",
    "            df['Number of Employees (year of last update)'].fillna(self.employee_mode, inplace=True)\n",
    "        if 'Number of Employees' in df.columns:\n",
    "            df['Number of Employees'] = df['Number of Employees'].apply(lambda x: 0 if pd.isna(x) or x < 0 else x)\n",
    "            df['Number of Employees'] = df['Number of Employees'].replace(0, self.mean_without_zeros)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class TaglineCategoryGuesser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_keywords = {'Artificial Intelligence': ['ai', 'machine learning', 'deep learning', 'neural network'], 'Mobile': ['mobile', 'android', 'ios', 'app store', 'smartphone'], 'E-Commerce': ['ecommerce', 'e-commerce', 'shopping', 'online store'], 'FinTech': ['finance', 'banking', 'payments', 'fintech', 'crypto', 'blockchain'], 'Healthcare': ['health', 'medical', 'hospital', 'doctor', 'pharma'], 'Social Media': ['social network', 'community', 'messaging', 'chat'], 'Gaming': ['game', 'gaming', 'video game', 'esports'], 'Cloud': ['cloud', 'saas', 'paas', 'infrastructure'], 'EdTech': ['education', 'learning', 'students', 'teaching', 'school'], 'Data Analytics': ['analytics', 'data science', 'big data', 'insights']}\n",
    "\n",
    "    def guess_category_from_tagline(self, tagline):\n",
    "        tagline = str(tagline).lower()\n",
    "        matched = [cat for (cat, keywords) in self.category_keywords.items() if any((keyword in tagline for keyword in keywords))]\n",
    "        if len(matched) == 0:\n",
    "            matched = ['Software', 'Advertising']\n",
    "        elif len(matched) == 1:\n",
    "            matched.append('Software')\n",
    "        return ', '.join(matched)\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['Tagline'] = df['Tagline'].fillna('')\n",
    "        df['Market Categories'] = df['Market Categories'].fillna('Unknown')\n",
    "        df['Market Categories'] = df.apply(lambda row: self.guess_category_from_tagline(row['Tagline']) if str(row['Market Categories']).strip().lower() in ['unknown', 'nan', 'none', ''] else row['Market Categories'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class MarketCategoryGeneralizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_mapping = {'Software': 'Technology & Software', 'Advertising': 'Advertising & Marketing', 'E-Commerce': 'E-Commerce & Online Services', 'Mobile': 'Mobile & Consumer Electronics', 'Games': 'Games & Entertainment', 'Social Media': 'Social Networking & Communication', 'Cloud': 'Technology & Software', 'Finance': 'Finance & Payments', 'Healthcare': 'Healthcare & Wellness', 'Semiconductors': 'Technology Hardware', 'Data Analytics': 'Analytics & Data Science', 'Search': 'Advertising & Marketing', 'Video': 'Games & Entertainment', 'Networking': 'Telecom & Networks', 'Messaging': 'Social Networking & Communication', 'Education': 'Education & Learning', 'News': 'Media & News', 'Photo Sharing': 'Digital Media & Content', 'Mobile Payments': 'Finance & Payments', 'Robotics': 'Games & Entertainment', 'Music': 'Games & Entertainment', 'Photo Editing': 'Digital Media & Content', 'Online Rental': 'E-Commerce & Online Services', 'Location Based Services': 'Telecom & Networks', 'Enterprise Software': 'Technology & Software', 'Video Streaming': 'Games & Entertainment', 'PaaS': 'Technology & Software', 'SaaS': 'Technology & Software', 'Health and Wellness': 'Healthcare & Wellness', 'Web Hosting': 'Technology & Software', 'Internet of Things': 'IoT (Internet of Things)', 'Cloud Security': 'Technology & Software', 'Virtual Currency': 'Finance & Payments', 'Search Marketing': 'Advertising & Marketing', 'Mobile Social': 'Social Networking & Communication', 'Retail': 'Retail & Fashion', 'Consulting': 'Others & Miscellaneous', 'Aerospace': 'Others & Miscellaneous', 'Food Delivery': 'Consumer Goods & Services', 'Fashion': 'Retail & Fashion', 'Wine And Spirits': 'Consumer Goods & Services', 'Streaming': 'Games & Entertainment', 'Task Management': 'Others & Miscellaneous', 'Video Chat': 'Social Networking & Communication', 'Personalization': 'Advertising & Marketing', 'Shopping': 'E-Commerce & Online Services', 'Local': 'E-Commerce & Online Services', 'News': 'Media & News', 'Fraud Detection': 'Advertising & Marketing', 'Image Recognition': 'Technology Hardware', 'Virtualization': 'Games & Entertainment', 'Analytics': 'Analytics & Data Science', 'Video on Demand': 'Games & Entertainment', 'Mobile Payments': 'Finance & Payments', 'Marketing Automation': 'Advertising & Marketing', 'Consumer Electronics': 'Mobile & Consumer Electronics', 'Video Games': 'Games & Entertainment', 'Public Relations': 'Advertising & Marketing'}\n",
    "\n",
    "    def map_categories(self, row):\n",
    "        categories = str(row).split(',')\n",
    "        generalized = []\n",
    "        for cat in categories:\n",
    "            cat = cat.strip()\n",
    "            if cat in self.category_mapping:\n",
    "                generalized.append(self.category_mapping[cat])\n",
    "            else:\n",
    "                generalized.append('Others & Miscellaneous')\n",
    "        return ', '.join(set(generalized))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df['Generalized Market Categories'] = df['Market Categories'].fillna('').apply(self.map_categories)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CountryRegionFiller:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.countries = [country.name for country in pycountry.countries]\n",
    "        self.regions = ['California', 'New York', 'Texas', 'Basel', 'Utah', 'ÃŽle-de-France', 'Bavaria', 'Ontario', 'Switzerland', 'United States', 'France', 'Great Britain', 'Israel', 'Sweden', 'Canada', 'Germany', 'Japan', 'India', 'Denmark', 'China', 'Spain', 'Netherlands', 'Finland', 'Australia', 'Ireland', 'United Stats of AMerica', 'United Arab Emirates', 'Quebec']\n",
    "\n",
    "    def find_place(self, text, place_list):\n",
    "        for place in place_list:\n",
    "            if re.search('\\\\b' + re.escape(place) + '\\\\b', str(text)):\n",
    "                return place\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for (idx, row) in df[df['Country (HQ)'].isnull() | df['State / Region (HQ)'].isnull()].iterrows():\n",
    "            desc = row['Description']\n",
    "            if pd.isnull(desc):\n",
    "                continue\n",
    "            country = self.find_place(desc, self.countries)\n",
    "            region = self.find_place(desc, self.regions)\n",
    "            if pd.isnull(row['Country (HQ)']) and country:\n",
    "                df.at[idx, 'Country (HQ)'] = country\n",
    "            if pd.isnull(row['State / Region (HQ)']) and region:\n",
    "                df.at[idx, 'State / Region (HQ)'] = region\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "class CategoricalFillerAndEncoder:\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.modes = {}\n",
    "        self.label_encoders = {}\n",
    "        self.label_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            mode_val = X[col].mode()[0]\n",
    "            self.modes[col] = mode_val\n",
    "            le = LabelEncoder()\n",
    "            filled = X[col].fillna(mode_val).astype(str)\n",
    "            le.fit(filled)\n",
    "            self.label_encoders[col] = le\n",
    "            self.label_maps[col] = {label: i for (i, label) in enumerate(le.classes_)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            mode_val = self.modes[col]\n",
    "            label_map = self.label_maps[col]\n",
    "            df[col] = df[col].fillna(mode_val).astype(str)\n",
    "            df[col + '_LabelEncoded'] = df[col].map(lambda x: label_map.get(x, -1))\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status_cols_ = None\n",
    "        self.terms_cols_ = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.status_cols_ = df['Status'].unique()\n",
    "        self.terms_cols_ = df['Terms'].unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df = pd.get_dummies(df, columns=['Status'], drop_first=False)\n",
    "        df = pd.get_dummies(df, columns=['Terms'], drop_first=False)\n",
    "        if 'Terms_Cash, Stock' in df.columns:\n",
    "            cash_stock_mask = df['Terms_Cash, Stock'] == 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
    "            df = df.drop('Terms_Cash, Stock', axis=1)\n",
    "        expected_cols = [f'Status_{s}' for s in self.status_cols_] + [f'Terms_{t}' for t in self.terms_cols_ if t != 'Cash, Stock']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df[expected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) [' Advertising Platforms', ' All Markets', ' All Students', ' Auctions', ' Blogging Platforms', ' Cloud Computing', ' Colleges', ' Communities', ' Consulting', ' Consumer Electronics', ' Consumer Goods', ' Content Creators', ' Creative', ' Crowdsourcing', ' Curated Web', ' Design', ' Digital Media', ' E-Commerce', ' Email', ' Enterprise Software', ' Enterprises', ' Facebook Applications', ' Hardware', ' Hardware + Software', ' Identity', ' Image Recognition', ' Information Technology', ' MicroBlogging', ' Mobile', ' Networking', ' Photography', ' RIM', ' SMS', ' Search', ' Security', ' Social Bookmarking', ' Social Media', ' Software', ' Storage', ' Technology', ' Video Streaming', ' Web Hosting', ' Wireless'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Software', 'Hardware + Software', 'Mobile', 'Social Media',\\n       'Curated Web', 'Web Hosting', 'Search', 'Networking',\\n       'Enterprise Software', 'Email', 'Security', 'Hardware',\\n       'Information Technology', 'E-Commerce', 'Electronics'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 258\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 258\u001b[0m     \u001b[43mpredict_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquiring.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquired_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquired.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquisitions_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquisitions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_predictions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 222\u001b[0m, in \u001b[0;36mpredict_new_data\u001b[0;34m(acquiring_path, acquired_path, acquisitions_path, output_path)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data \n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Preprocess each new dataset\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m acquiring_new \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_acquiring_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m acquired_new \u001b[38;5;241m=\u001b[39m process_acquired_new(pd\u001b[38;5;241m.\u001b[39mread_csv(acquired_path)) \n\u001b[1;32m    224\u001b[0m acquisitions_new \u001b[38;5;241m=\u001b[39m process_acquisitions_new(pd\u001b[38;5;241m.\u001b[39mread_csv(acquisitions_path))\n",
      "Cell \u001b[0;32mIn[6], line 124\u001b[0m, in \u001b[0;36mpredict_new_data.<locals>.process_acquiring_new\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m loaded_filter\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Use the loaded reducer on new data\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_reducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    127\u001b[0m data \u001b[38;5;241m=\u001b[39m age_mode_col\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mCategoryReducer.transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m---> 34\u001b[0m     df_top \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_categories\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     35\u001b[0m     other_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_categories))\n\u001b[1;32m     36\u001b[0m     df_top[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[other_columns]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Software', 'Hardware + Software', 'Mobile', 'Social Media',\\n       'Curated Web', 'Web Hosting', 'Search', 'Networking',\\n       'Enterprise Software', 'Email', 'Security', 'Hardware',\\n       'Information Technology', 'E-Commerce', 'Electronics'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    \"\"\"\n",
    "    Process new data and make predictions using saved models\n",
    "    \n",
    "    Args:\n",
    "        acquiring_path: Path to new acquiring companies CSV\n",
    "        acquired_path: Path to new acquired companies CSV  \n",
    "        acquisitions_path: Path to new acquisitions CSV\n",
    "        output_path: Where to save predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================================\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    # ===================================\n",
    "    \n",
    "    # Load acquiring company transformers\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    \n",
    "    with open(\"correlation_filte_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_filter = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    with open(\"category_reducer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_reducer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open('Age_column_acquiring.pkl', 'rb') as f:\n",
    "        age_mode_col =  pickle.load(f)\n",
    "    #\n",
    "    with open(\"ipo_transformer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_ipo_transformer = pickle.load(f)\n",
    "    #\n",
    "    with open(\"employee_cleaner_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_employee_cleaner = pickle.load(f)\n",
    "\n",
    "\n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "        \n",
    "    #\n",
    "    with open(\"tagline_guesser_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_guesser = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"category_generalizer_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_generalizer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"country_region_filler_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_filler = pickle.load(f)\n",
    "    \n",
    "    #\n",
    "    with open(\"categorical_encoder_acquired.pkl\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    # Load acquired company transformers\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    # with open('label_encoders_acquired.pkl', 'rb') as f:\n",
    "    #     label_encoders_acquired = pickle.load(f)\n",
    "        \n",
    "    # Load acquisitions transformers\n",
    "    # with open('ohe_acquisitions.pkl', 'rb') as f:\n",
    "    #     ohe_acquisitions = pickle.load(f)\n",
    "        \n",
    "    # Load final preprocessing objects\n",
    "    # with open('final_imputer.pkl', 'rb') as f:\n",
    "    #     final_imputer = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Load saved preprocessing objects\n",
    "    \n",
    "    # Load model\n",
    "    # with open('final_model.pkl', 'rb') as f:\n",
    "    #     model = pickle.load(f)\n",
    "    \n",
    "    # ==============================\n",
    "    # Preprocess New Data (Same as Training)\n",
    "    # ==============================\n",
    "    \n",
    "    # Process each dataset with saved transformers\n",
    "    def process_acquiring_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Apply same cleaning as training\n",
    "        data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'], \n",
    "                 axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        \n",
    "        # Use mean from training (would need to save this)\n",
    "        data['Number of Employees'] = data['Number of Employees'].replace(\n",
    "            0, 450)  # Replace with saved mean from training\n",
    "        \n",
    "        # Handle IPO status\n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data.drop('IPO', axis=1, inplace=True)\n",
    "\n",
    "        # Process Market Categories with saved MLB\n",
    "        data['Market Categories'] = data['Market Categories'].fillna('')\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories'].str.split(',')),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        data = loaded_filter.transform(data)\n",
    "\n",
    "\n",
    "        # Use the loaded reducer on new data\n",
    "        data = loaded_reducer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = age_mode_col.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_ipo_transformer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_employee_cleaner.transform(data)\n",
    "\n",
    "\n",
    "\n",
    "        # Process text with saved TF-IDF\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
    "            return text\n",
    "\n",
    "        data['Text_Combined'] = data['Text_Combined'].apply(clean_text)\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Final cleanup\n",
    "        data = pd.concat([data, category_dummies, tfidf_df], axis=1)\n",
    "        data.drop(['Market Categories', 'Tagline', 'Description', 'Text_Combined','Address (HQ)',\n",
    "                   'Board Members','Founders'], \n",
    "                 axis=1, inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Similar functions for acquired and acquisitions...\n",
    "    # (Implementation would mirror the training preprocessing but using saved transformers)\n",
    "    \n",
    "\n",
    "    def process_acquisitions_new(data):\n",
    "        \"\"\"Process new acquisitions data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        \n",
    "        # Drop columns same as training\n",
    "        data.drop(columns=[\"Acquisition Profile\", \"News\", \"News Link\"], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        with open('custom_acquisitions_encoder.pkl', 'rb') as f:\n",
    "            encoder = pickle.load(f)\n",
    "        with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "            modes = pickle.load(f)\n",
    "\n",
    "        # Handle missing values with saved modes\n",
    "        for col, mode_val in modes.items():\n",
    "            if col in data.columns and mode_val is not None:\n",
    "                data[col].fillna(mode_val, inplace=True)\n",
    "        \n",
    "        # Convert date features same as training\n",
    "        if 'Deal announced on' in data.columns:\n",
    "            data['Deal_date'] = pd.to_datetime(data['Deal announced on'], dayfirst=True, errors='coerce')\n",
    "            data['Deal_day'] = data['Deal_date'].dt.day\n",
    "            data['Deal_month'] = data['Deal_date'].dt.month\n",
    "            data['Deal_dayofweek'] = data['Deal_date'].dt.dayofweek\n",
    "            data.drop(['Deal announced on', 'Deal_date'], axis=1, inplace=True)\n",
    "        \n",
    "        # Apply saved one-hot encoding\n",
    "        encoded_status_terms = encoder.transform(data)\n",
    "        data = pd.concat([data, encoded_status_terms], axis=1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process_acquired_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "\n",
    "        data = loaded_guesser.transform(data)\n",
    "\n",
    "        data = loaded_generalizer.transform(data)\n",
    "\n",
    "        data = loaded_filler.transform(data)\n",
    "\n",
    "        data = encoder.transform(data)\n",
    "        \n",
    "        data['Acquired by'].fillna('Salesforce', inplace=True)\n",
    "\n",
    "\n",
    "        columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter','Address (HQ)','API','Description',\n",
    "                           'Tagline','Market Categories','City (HQ)', 'State / Region (HQ)', 'Country (HQ)','Generalized Market Categories','Year Founded']\n",
    "        data.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Preprocess each new dataset\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path)) \n",
    "    acquisitions_new = process_acquisitions_new(pd.read_csv(acquisitions_path))\n",
    "    \n",
    "    # Merge the new data (same as training)\n",
    "    final_new = merge_datasets(\n",
    "        acquiring_new, acquired_new, acquisitions_new, save_artifacts=False\n",
    "    )\n",
    "    \n",
    "    # Handle missing values with saved imputer\n",
    "    final_new_imputed = pd.DataFrame(\n",
    "        final_imputer.transform(final_new),\n",
    "        columns=final_new.columns\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    X_new = final_new_imputed.drop(\n",
    "        ['Deal size class', 'Acquired Company', 'Acquiring Company'], \n",
    "        axis=1, errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Apply same scaling and PCA as training\n",
    "    X_new_scaled = final_scaler.transform(X_new)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Save predictions with original data\n",
    "    final_new['Predicted_Deal_Size'] = predictions\n",
    "    final_new.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def browse_file(entry_widget):\n",
    "    directory = filedialog.askopenfilename()\n",
    "    if directory:\n",
    "        entry_widget.delete(0, tk.END)\n",
    "        entry_widget.insert(0, directory)\n",
    "\n",
    "def predict_data():\n",
    "    # Get input values from entries\n",
    "    paths = [\n",
    "        entry1.get(),\n",
    "        entry2.get(),\n",
    "        entry3.get()\n",
    "    ]\n",
    "    \n",
    "    # Validate paths\n",
    "    for path in paths:\n",
    "        if not path.strip():\n",
    "            messagebox.showerror(\"Error\", \"All paths must be selected!\")\n",
    "            return\n",
    "    \n",
    "    # Call your prediction function\n",
    "    try:\n",
    "        predict_new_data(*paths, \"new_predictions.csv\")\n",
    "        messagebox.showinfo(\"Success\", \"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Predictor\")\n",
    "root.geometry(\"1200x800\")\n",
    "\n",
    "# Function to create consistent input fields with browse buttons\n",
    "def create_file_input(row, label_text):\n",
    "    # Label\n",
    "    tk.Label(root, text=label_text).grid(row=row, column=0, padx=10, pady=5, sticky=tk.W)\n",
    "    \n",
    "    # Entry field\n",
    "    entry = tk.Entry(root, width=80)\n",
    "    entry.grid(row=row, column=1, padx=10, pady=5)\n",
    "    \n",
    "    # Browse button\n",
    "    browse_btn = tk.Button(root, text=\"Browse\", \n",
    "                          command=lambda: browse_file(entry))\n",
    "    browse_btn.grid(row=row, column=2, padx=10, pady=5)\n",
    "    \n",
    "    return entry\n",
    "\n",
    "# Create input fields with browse buttons\n",
    "entry1 = create_file_input(0, \"Acquiring Path:\")\n",
    "entry2 = create_file_input(1, \"Acquired Path:\")\n",
    "entry3 = create_file_input(2, \"Acquisitions Path:\")\n",
    "\n",
    "# Prediction button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_data,\n",
    "                          bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "predict_button.grid(row=3, column=0, columnspan=3, pady=20, ipadx=10, ipady=5)\n",
    "\n",
    "# Configure grid layout\n",
    "root.grid_columnconfigure(1, weight=1)  # Make entry fields expandable\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
