{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698874ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "from collections import Counter\n",
    "from joblib import dump\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pycountry\n",
    "import re\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COLUMN_ALIASES = {\n",
    "    'IPO Date': 'IPO',\n",
    "    'Went Public': 'IPO',\n",
    "    'Public Listing': 'IPO'\n",
    "}\n",
    "\n",
    "def ensure_ipo(df):\n",
    "    df.rename(columns=COLUMN_ALIASES, inplace=True)\n",
    "    if 'IPO' not in df.columns:\n",
    "        df['IPO'] = np.nan\n",
    "    return df\n",
    "# ======================== CLASSES ========================\n",
    "class CorrelationFilter:\n",
    "    def __init__(self, threshold=0.85):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = set()  # Stores columns to drop\n",
    "        self.fitted = False   # Tracks if fit() was called\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identifies highly correlated columns to drop.\"\"\"\n",
    "        numerical_data = data.select_dtypes(include=[np.number])\n",
    "        corr_matrix = numerical_data.corr()\n",
    "\n",
    "        self.to_drop = set()  # Reset in case fit() is called again\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    self.to_drop.add(colname)\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Drops columns identified in fit().\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "        \n",
    "        cols_to_drop = list(self.to_drop & set(data.columns))\n",
    "        return data.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        \"\"\"Combines fit() and transform().\"\"\"\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    def get_columns_to_drop(self):\n",
    "        \"\"\"Returns the list of columns to be dropped.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() first!\")\n",
    "        return list(self.to_drop)\n",
    "    \n",
    "class CategoryReducer:\n",
    "    def __init__(self, category_columns, top_n=15):\n",
    "        self.category_columns = category_columns\n",
    "        self.top_n = top_n\n",
    "        self.top_categories = None  # Will store the top categories from training\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Identify and store the top N categories (only during training)\n",
    "        self.top_categories = (\n",
    "            data[self.category_columns]\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(self.top_n)\n",
    "            .index.tolist()\n",
    "        )\n",
    "        return self  # For sklearn compatibility\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.top_categories is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        # Keep only the top categories (from training)\n",
    "        df_top = data[self.top_categories].copy()\n",
    "\n",
    "        # Sum remaining categories into \"Other\"\n",
    "        other_columns = list(set(self.category_columns) - set(self.top_categories))\n",
    "        df_top['Other'] = data[other_columns].sum(axis=1).clip(upper=1)  # Ensures 0 or 1\n",
    "\n",
    "        # Drop original columns and concatenate reduced set\n",
    "        data = data.drop(columns=self.category_columns)\n",
    "        return pd.concat([data, df_top], axis=1)\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    # Handle pickle compatibility\n",
    "    def __getstate__(self):\n",
    "        return {k: v for k, v in self.__dict__.items() \n",
    "                if k in ['category_columns', 'top_n', 'top_categories',\n",
    "                         'original_categories', 'expected_columns']}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        # Initialize missing attributes for old versions\n",
    "        if not hasattr(self, 'original_categories'):\n",
    "            self.original_categories = self.category_columns\n",
    "        if not hasattr(self, 'expected_columns'):\n",
    "            self.expected_columns = (self.top_categories + ['Other'] \n",
    "                                     if self.top_categories else None)\n",
    "class AgeTransformer:\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "        self.age_mode = None\n",
    "        self.column_exists = True  # Track if column existed during training\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Check if column exists in training data\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            self.column_exists = False\n",
    "            return self\n",
    "            \n",
    "        data = data.copy()\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        mode_series = data['age'].mode()\n",
    "        self.age_mode = mode_series[0] if not mode_series.empty else 5\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Handle missing 'Year Founded' column gracefully\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Create column if it doesn't exist\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            if self.column_exists:\n",
    "                # Column existed in training but missing in new data\n",
    "                data['Year Founded'] = np.nan\n",
    "            else:\n",
    "                # Column never existed (new scenario)\n",
    "                data['age'] = self.age_mode\n",
    "                return data\n",
    "                \n",
    "        # Original processing if column exists\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        data.drop(columns=['Year Founded'], inplace=True, errors='ignore')\n",
    "        data['age'].fillna(self.age_mode, inplace=True)\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class IPOAgeTransformer:\n",
    "    def __init__(self, current_year=2025, unknown_placeholder=\"Unknown\"):\n",
    "        self.current_year = current_year\n",
    "        self.unknown_placeholder = unknown_placeholder  # Replace NaN values\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Stateless (no training needed). For pipeline compatibility.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Computes IPO age and replaces missing values.\"\"\"\n",
    "        # if 'IPO' not in data.columns:\n",
    "        #     raise ValueError(\"Column 'IPO' not found in data.\")\n",
    "\n",
    "        df = data.copy()\n",
    "        df = ensure_ipo(df)\n",
    "        df['IPO'] = pd.to_numeric(df['IPO'], errors='coerce')\n",
    "        \n",
    "        # Compute age (clamp negative values to 0)\n",
    "        df['age IPO'] = (self.current_year - df['IPO']).clip(lower=0)\n",
    "        \n",
    "        # Replace missing ages with placeholder\n",
    "        df['age IPO'] = df['age IPO'].replace(\n",
    "            np.nan, self.unknown_placeholder\n",
    "        )\n",
    "        \n",
    "        # Drop original IPO column\n",
    "        df.drop(columns=['IPO'], inplace=True, errors='ignore')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.transform(data)  # fit() is stateless\n",
    "    \n",
    "class EmployeeDataCleaner:\n",
    "    def __init__(self):\n",
    "        self.employee_mode = None\n",
    "        self.mean_without_zeros = None\n",
    "        self.fitted = False  # Safety flag\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Compute and store statistics from training data.\"\"\"\n",
    "        # Validate columns\n",
    "        required_columns = [\n",
    "            'Number of Employees (year of last update)',\n",
    "            'Number of Employees'\n",
    "        ]\n",
    "        for col in required_columns:\n",
    "            if col not in data.columns:\n",
    "                raise ValueError(f\"Column '{col}' not found in data.\")\n",
    "\n",
    "        # Compute mode for 'Number of Employees (year of last update)'\n",
    "        mode_series = data['Number of Employees (year of last update)'].mode()\n",
    "        self.employee_mode = mode_series[0] if not mode_series.empty else 0\n",
    "\n",
    "        # Compute mean (excluding zeros/negatives) for 'Number of Employees'\n",
    "        non_zero_employees = data.loc[\n",
    "            data['Number of Employees'] > 0, 'Number of Employees'\n",
    "        ]\n",
    "        self.mean_without_zeros = non_zero_employees.mean()\n",
    "\n",
    "        # Fallback if all values are zero/NaN\n",
    "        if pd.isna(self.mean_without_zeros):\n",
    "            self.mean_without_zeros = data['Number of Employees'].median()  # or a global default\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Apply cleaning using statistics from fit().\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "\n",
    "        # Fill missing values with training mode\n",
    "        if 'Number of Employees (year of last update)' in df.columns:\n",
    "            df['Number of Employees (year of last update)'].fillna(\n",
    "                self.employee_mode, inplace=True\n",
    "            )\n",
    "\n",
    "        # Handle nulls/negatives and replace zeros with training mean\n",
    "        if 'Number of Employees' in df.columns:\n",
    "            df['Number of Employees'] = np.where(\n",
    "                df['Number of Employees'].isna() | (df['Number of Employees'] < 0),\n",
    "                0,\n",
    "                df['Number of Employees']\n",
    "            )\n",
    "            df['Number of Employees'] = df['Number of Employees'].replace(\n",
    "                0, self.mean_without_zeros\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class BoardMembersTransformer:\n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count  # Keep only members appearing ≥ min_count times\n",
    "        self.common_members = None  # Stores frequent members from training\n",
    "        self.member_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring board members from training data.\"\"\"\n",
    "        if 'Board Members' not in data.columns:\n",
    "            raise ValueError(\"Column 'Board Members' not found in data.\")\n",
    "\n",
    "        all_members = []\n",
    "        for cell in data['Board Members'].dropna():\n",
    "            members = [name.strip() for name in str(cell).split(',')]\n",
    "            all_members.extend(members)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.member_counts_ = Counter(all_members)\n",
    "        self.common_members = {\n",
    "            name for name, count in self.member_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert board members into binary features for common members.\"\"\"\n",
    "        if self.common_members is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common member\n",
    "        for member in self.common_members:\n",
    "            df[f'Board Member: {member}'] = df['Board Members'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and member in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature (total members or binary \"has members\")\n",
    "        df['Has Board Members'] = df['Board Members'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Board Members'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class BoardMembersTransformer:\n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count  # Keep only members appearing ≥ min_count times\n",
    "        self.common_members = None  # Stores frequent members from training\n",
    "        self.member_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring board members from training data.\"\"\"\n",
    "        if 'Board Members' not in data.columns:\n",
    "            raise ValueError(\"Column 'Board Members' not found in data.\")\n",
    "\n",
    "        all_members = []\n",
    "        for cell in data['Board Members'].dropna():\n",
    "            members = [name.strip() for name in str(cell).split(',')]\n",
    "            all_members.extend(members)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.member_counts_ = Counter(all_members)\n",
    "        self.common_members = {\n",
    "            name for name, count in self.member_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert board members into binary features for common members.\"\"\"\n",
    "        if self.common_members is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common member\n",
    "        for member in self.common_members:\n",
    "            df[f'Board Member: {member}'] = df['Board Members'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and member in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature (total members or binary \"has members\")\n",
    "        df['Has Board Members'] = df['Board Members'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Board Members'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class FoundersTransformer:\n",
    "    def __init__(self, min_count=3):\n",
    "        self.min_count = min_count  # Keep founders appearing ≥ min_count times\n",
    "        self.common_founders = None  # Stores frequent founders from training\n",
    "        self.founder_counts_ = None  # Optional: track raw counts\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Identify frequently occurring founders from training data.\"\"\"\n",
    "        if 'Founders' not in data.columns:\n",
    "            raise ValueError(\"Column 'Founders' not found in data.\")\n",
    "\n",
    "        all_founders = []\n",
    "        for cell in data['Founders'].dropna():\n",
    "            founders = [name.strip() for name in str(cell).split(',')]\n",
    "            all_founders.extend(founders)\n",
    "\n",
    "        # Count occurrences and filter by min_count\n",
    "        self.founder_counts_ = Counter(all_founders)\n",
    "        self.common_founders = {\n",
    "            name for name, count in self.founder_counts_.items() \n",
    "            if count >= self.min_count\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Convert founders into binary features for common founders.\"\"\"\n",
    "        if self.common_founders is None:\n",
    "            raise RuntimeError(\"Call fit() before transform()!\")\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # Create binary columns for each common founder\n",
    "        for founder in self.common_founders:\n",
    "            df[f'Founder: {founder}'] = df['Founders'].apply(\n",
    "                lambda x: 1 if pd.notna(x) and founder in str(x) else 0\n",
    "            )\n",
    "\n",
    "        # Optional: Add a summary feature\n",
    "        df['Has Founders'] = df['Founders'].notna().astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        df.drop(columns=['Founders'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "class TaglineCategoryGuesser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_keywords = {'Artificial Intelligence': ['ai', 'machine learning', 'deep learning', 'neural network'], 'Mobile': ['mobile', 'android', 'ios', 'app store', 'smartphone'], 'E-Commerce': ['ecommerce', 'e-commerce', 'shopping', 'online store'], 'FinTech': ['finance', 'banking', 'payments', 'fintech', 'crypto', 'blockchain'], 'Healthcare': ['health', 'medical', 'hospital', 'doctor', 'pharma'], 'Social Media': ['social network', 'community', 'messaging', 'chat'], 'Gaming': ['game', 'gaming', 'video game', 'esports'], 'Cloud': ['cloud', 'saas', 'paas', 'infrastructure'], 'EdTech': ['education', 'learning', 'students', 'teaching', 'school'], 'Data Analytics': ['analytics', 'data science', 'big data', 'insights']}\n",
    "\n",
    "    def guess_category_from_tagline(self, tagline):\n",
    "        tagline = str(tagline).lower()\n",
    "        matched = [cat for (cat, keywords) in self.category_keywords.items() if any((keyword in tagline for keyword in keywords))]\n",
    "        if len(matched) == 0:\n",
    "            matched = ['Software', 'Advertising']\n",
    "        elif len(matched) == 1:\n",
    "            matched.append('Software')\n",
    "        return ', '.join(matched)\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['Tagline'] = df['Tagline'].fillna('')\n",
    "        df['Market Categories'] = df['Market Categories'].fillna('Unknown')\n",
    "        df['Market Categories'] = df.apply(lambda row: self.guess_category_from_tagline(row['Tagline']) if str(row['Market Categories']).strip().lower() in ['unknown', 'nan', 'none', ''] else row['Market Categories'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class MarketCategoryGeneralizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_mapping = {'Software': 'Technology & Software', 'Advertising': 'Advertising & Marketing', 'E-Commerce': 'E-Commerce & Online Services', 'Mobile': 'Mobile & Consumer Electronics', 'Games': 'Games & Entertainment', 'Social Media': 'Social Networking & Communication', 'Cloud': 'Technology & Software', 'Finance': 'Finance & Payments', 'Healthcare': 'Healthcare & Wellness', 'Semiconductors': 'Technology Hardware', 'Data Analytics': 'Analytics & Data Science', 'Search': 'Advertising & Marketing', 'Video': 'Games & Entertainment', 'Networking': 'Telecom & Networks', 'Messaging': 'Social Networking & Communication', 'Education': 'Education & Learning', 'News': 'Media & News', 'Photo Sharing': 'Digital Media & Content', 'Mobile Payments': 'Finance & Payments', 'Robotics': 'Games & Entertainment', 'Music': 'Games & Entertainment', 'Photo Editing': 'Digital Media & Content', 'Online Rental': 'E-Commerce & Online Services', 'Location Based Services': 'Telecom & Networks', 'Enterprise Software': 'Technology & Software', 'Video Streaming': 'Games & Entertainment', 'PaaS': 'Technology & Software', 'SaaS': 'Technology & Software', 'Health and Wellness': 'Healthcare & Wellness', 'Web Hosting': 'Technology & Software', 'Internet of Things': 'IoT (Internet of Things)', 'Cloud Security': 'Technology & Software', 'Virtual Currency': 'Finance & Payments', 'Search Marketing': 'Advertising & Marketing', 'Mobile Social': 'Social Networking & Communication', 'Retail': 'Retail & Fashion', 'Consulting': 'Others & Miscellaneous', 'Aerospace': 'Others & Miscellaneous', 'Food Delivery': 'Consumer Goods & Services', 'Fashion': 'Retail & Fashion', 'Wine And Spirits': 'Consumer Goods & Services', 'Streaming': 'Games & Entertainment', 'Task Management': 'Others & Miscellaneous', 'Video Chat': 'Social Networking & Communication', 'Personalization': 'Advertising & Marketing', 'Shopping': 'E-Commerce & Online Services', 'Local': 'E-Commerce & Online Services', 'News': 'Media & News', 'Fraud Detection': 'Advertising & Marketing', 'Image Recognition': 'Technology Hardware', 'Virtualization': 'Games & Entertainment', 'Analytics': 'Analytics & Data Science', 'Video on Demand': 'Games & Entertainment', 'Mobile Payments': 'Finance & Payments', 'Marketing Automation': 'Advertising & Marketing', 'Consumer Electronics': 'Mobile & Consumer Electronics', 'Video Games': 'Games & Entertainment', 'Public Relations': 'Advertising & Marketing'}\n",
    "\n",
    "    def map_categories(self, row):\n",
    "        categories = str(row).split(',')\n",
    "        generalized = []\n",
    "        for cat in categories:\n",
    "            cat = cat.strip()\n",
    "            if cat in self.category_mapping:\n",
    "                generalized.append(self.category_mapping[cat])\n",
    "            else:\n",
    "                generalized.append('Others & Miscellaneous')\n",
    "        return ', '.join(set(generalized))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df['Generalized Market Categories'] = df['Market Categories'].fillna('').apply(self.map_categories)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CountryRegionFiller:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.countries = [country.name for country in pycountry.countries]\n",
    "        self.regions = ['California', 'New York', 'Texas', 'Basel', 'Utah', 'Île-de-France', 'Bavaria', 'Ontario', 'Switzerland', 'United States', 'France', 'Great Britain', 'Israel', 'Sweden', 'Canada', 'Germany', 'Japan', 'India', 'Denmark', 'China', 'Spain', 'Netherlands', 'Finland', 'Australia', 'Ireland', 'United Stats of AMerica', 'United Arab Emirates', 'Quebec']\n",
    "\n",
    "    def find_place(self, text, place_list):\n",
    "        for place in place_list:\n",
    "            if re.search('\\\\b' + re.escape(place) + '\\\\b', str(text)):\n",
    "                return place\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for (idx, row) in df[df['Country (HQ)'].isnull() | df['State / Region (HQ)'].isnull()].iterrows():\n",
    "            desc = row['Description']\n",
    "            if pd.isnull(desc):\n",
    "                continue\n",
    "            country = self.find_place(desc, self.countries)\n",
    "            region = self.find_place(desc, self.regions)\n",
    "            if pd.isnull(row['Country (HQ)']) and country:\n",
    "                df.at[idx, 'Country (HQ)'] = country\n",
    "            if pd.isnull(row['State / Region (HQ)']) and region:\n",
    "                df.at[idx, 'State / Region (HQ)'] = region\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "class CategoricalFillerAndEncoder:\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.modes = {}\n",
    "        self.label_encoders = {}\n",
    "        self.label_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            mode_val = X[col].mode()[0]\n",
    "            self.modes[col] = mode_val\n",
    "            le = LabelEncoder()\n",
    "            filled = X[col].fillna(mode_val).astype(str)\n",
    "            le.fit(filled)\n",
    "            self.label_encoders[col] = le\n",
    "            self.label_maps[col] = {label: i for (i, label) in enumerate(le.classes_)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            mode_val = self.modes[col]\n",
    "            label_map = self.label_maps[col]\n",
    "            df[col] = df[col].fillna(mode_val).astype(str)\n",
    "            df[col + '_LabelEncoded'] = df[col].map(lambda x: label_map.get(x, -1))\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status_cols_ = None\n",
    "        self.terms_cols_ = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.status_cols_ = df['Status'].unique()\n",
    "        self.terms_cols_ = df['Terms'].unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df = pd.get_dummies(df, columns=['Status'], drop_first=False)\n",
    "        df = pd.get_dummies(df, columns=['Terms'], drop_first=False)\n",
    "        if 'Terms_Cash, Stock' in df.columns:\n",
    "            cash_stock_mask = df['Terms_Cash, Stock'] == 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
    "            df = df.drop('Terms_Cash, Stock', axis=1)\n",
    "        expected_cols = [f'Status_{s}' for s in self.status_cols_] + [f'Terms_{t}' for t in self.terms_cols_ if t != 'Cash, Stock']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df[expected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) [' Advertising Platforms', ' All Markets', ' All Students', ' Auctions', ' Blogging Platforms', ' Cloud Computing', ' Colleges', ' Communities', ' Consulting', ' Consumer Electronics', ' Consumer Goods', ' Content Creators', ' Creative', ' Crowdsourcing', ' Curated Web', ' Design', ' Digital Media', ' E-Commerce', ' Email', ' Enterprise Software', ' Enterprises', ' Facebook Applications', ' Hardware', ' Hardware + Software', ' Identity', ' Image Recognition', ' Information Technology', ' MicroBlogging', ' Mobile', ' Networking', ' Photography', ' RIM', ' SMS', ' Search', ' Security', ' Social Bookmarking', ' Social Media', ' Software', ' Storage', ' Technology', ' Video Streaming', ' Web Hosting', ' Wireless'] will be ignored\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_264689/4037191763.py:141: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['age'].fillna(self.age_mode, inplace=True)\n",
      "/tmp/ipykernel_264689/4037191763.py:224: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Number of Employees (year of last update)'].fillna(\n",
      "/tmp/ipykernel_264689/3923052822.py:250: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['Acquired by'].fillna('Salesforce', inplace=True)\n",
      "/tmp/ipykernel_264689/3923052822.py:222: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(mode_val, inplace=True)\n",
      "/tmp/ipykernel_264689/4037191763.py:515: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
      "/tmp/ipykernel_264689/4037191763.py:516: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
      "/tmp/ipykernel_264689/3923052822.py:318: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  final_df[col].fillna(mode_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Acquisitions ID Acquired Company  \\\n",
      "0     EMC acquired Data Domain in 2009      data domain   \n",
      "1           AOL acquired Quigo in 2007            quigo   \n",
      "2      Cisco acquired PostPath in 2008         postpath   \n",
      "3  Oracle acquired BigMachines in 2013      bigmachines   \n",
      "4      Yahoo! acquired Snip.it in 2013          snip.it   \n",
      "\n",
      "   Year of acquisition announcement Deal size class       Status        Terms  \\\n",
      "0                              2009           Large  Undisclosed         Cash   \n",
      "1                              2007          Medium  Undisclosed         Cash   \n",
      "2                              2008          Medium  Undisclosed  Undisclosed   \n",
      "3                              2013          Medium  Undisclosed  Undisclosed   \n",
      "4                              2013           Small  Undisclosed  Cash, Stock   \n",
      "\n",
      "   Deal_day  Deal_month  Deal_dayofweek  Status_Undisclosed  ...       web  \\\n",
      "0       8.0         7.0             2.0                True  ...  0.000000   \n",
      "1       7.0        11.0             2.0                True  ...  0.120831   \n",
      "2      27.0         8.0             2.0                True  ...  0.000000   \n",
      "3      24.0        10.0             3.0                True  ...  0.000000   \n",
      "4      22.0         1.0             1.0                True  ...  0.136166   \n",
      "\n",
      "   which wireless      with     work     world worlds  worldwide     yahoo  \\\n",
      "0    0.0      0.0  0.000000  0.00000  0.000000    0.0   0.000000  0.000000   \n",
      "1    0.0      0.0  0.046458  0.00000  0.042242    0.0   0.038519  0.000000   \n",
      "2    0.0      0.0  0.000000  0.00000  0.000000    0.0   0.000000  0.000000   \n",
      "3    0.0      0.0  0.045655  0.19688  0.000000    0.0   0.000000  0.000000   \n",
      "4    0.0      0.0  0.026177  0.00000  0.000000    0.0   0.000000  0.660479   \n",
      "\n",
      "   zuckerberg  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "\n",
      "[5 rows x 255 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'City (HQ)_Acquiring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:191\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:234\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:242\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:134\u001b[0m, in \u001b[0;36mpandas._libs.index._unpack_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'City (HQ)_Acquiring'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 346\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 346\u001b[0m     \u001b[43mpredict_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquiring.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquired_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquired.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquisitions_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquisitions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_predictions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 318\u001b[0m, in \u001b[0;36mpredict_new_data\u001b[0;34m(acquiring_path, acquired_path, acquisitions_path, output_path)\u001b[0m\n\u001b[1;32m    314\u001b[0m     modes \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, mode_val \u001b[38;5;129;01min\u001b[39;00m modes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    317\u001b[0m    \u001b[38;5;66;03m# if col in df.columns and mode_val is not None:\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m        \u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(mode_val, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Prepare features\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# X_new = final_df.drop(\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m#     ['Deal size class', 'Acquired Company', 'Acquiring Company'], \u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#     axis=1, errors='ignore'\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_features.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'City (HQ)_Acquiring'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    \"\"\"\n",
    "    Process new data and make predictions using saved models\n",
    "    \n",
    "    Args:\n",
    "        acquiring_path: Path to new acquiring companies CSV\n",
    "        acquired_path: Path to new acquired companies CSV  \n",
    "        acquisitions_path: Path to new acquisitions CSV\n",
    "        output_path: Where to save predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================================\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    # ===================================\n",
    "    \n",
    "    # Load acquiring company transformers\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    \n",
    "    # Load the saved filter\n",
    "    with open(\"correlation_filter.pkl\", \"rb\") as f:\n",
    "        loaded_filter = pickle.load(f)\n",
    "    \n",
    "    #\n",
    "    with open(\"category_reducer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_reducer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open('Age_column_acquiring.pkl', 'rb') as f:\n",
    "        age_mode_col =  pickle.load(f)\n",
    "    #\n",
    "    with open(\"ipo_transformer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_ipo_transformer = pickle.load(f)\n",
    "    #\n",
    "    with open(\"employee_cleaner_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_employee_cleaner = pickle.load(f)\n",
    "\n",
    "    # Load the saved transformer,\n",
    "    with open(\"board_members_transformer.pkl\", \"rb\") as f:\n",
    "        loaded_transformer = pickle.load(f)\n",
    "\n",
    "    # Load the saved transformer\n",
    "    with open(\"founders_transformer.pkl\", \"rb\") as f:\n",
    "        loaded_transformer = pickle.load(f)\n",
    "\n",
    "    \n",
    "    \n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "        \n",
    "    #\n",
    "    with open(\"tagline_guesser_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_guesser = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"category_generalizer_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_generalizer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"country_region_filler_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_filler = pickle.load(f)\n",
    "    \n",
    "    #\n",
    "    with open(\"categorical_encoder_acquired.pkl\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    # Load acquired company transformers\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    # with open('label_encoders_acquired.pkl', 'rb') as f:\n",
    "    #     label_encoders_acquired = pickle.load(f)\n",
    "        \n",
    "    # Load acquisitions transformers\n",
    "    # with open('ohe_acquisitions.pkl', 'rb') as f:\n",
    "    #     ohe_acquisitions = pickle.load(f)\n",
    "        \n",
    "    # Load final preprocessing objects\n",
    "    # with open('final_imputer.pkl', 'rb') as f:\n",
    "    #     final_imputer = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Load saved preprocessing objects\n",
    "    \n",
    "    # Load model\n",
    "    # with open('final_model.pkl', 'rb') as f:\n",
    "    #     model = pickle.load(f)\n",
    "    \n",
    "    # ==============================\n",
    "    # Preprocess New Data (Same as Training)\n",
    "    # ==============================\n",
    "    \n",
    "    # Process each dataset with saved transformers\n",
    "    def process_acquiring_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Apply same cleaning as training\n",
    "        data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'], \n",
    "                 axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype == 'object':\n",
    "                try:\n",
    "                    data[col] = data[col].astype(int)\n",
    "                except:\n",
    "                    pass  # ignore if it fails, i.e., for non-numeric text\n",
    "        # Use mean from training (would need to save this)\n",
    "        # data['Number of Employees'] = data['Number of Employees'].replace(\n",
    "        #     0, 450)  # Replace with saved mean from training\n",
    "        \n",
    "        # Handle IPO status\n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        data.drop('IPO', axis=1, inplace=True)\n",
    "        \n",
    "        # Process Market Categories with saved MLB\n",
    "        \n",
    "    \n",
    "\n",
    "        # instantiate and fit once\n",
    "\n",
    "        # 1. build the list column so it’s available later\n",
    "        data['Market Categories List'] = (\n",
    "            data['Market Categories']\n",
    "                .fillna('')          # protect against NaNs\n",
    "                .str.split(',')      # convert to Python list\n",
    "        )\n",
    "\n",
    "        # 2. create the one-hot columns from that list column\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories List']),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "         index=data.index\n",
    "     )\n",
    "        data = pd.concat([data, category_dummies], axis=1)\n",
    "        data.drop(columns=['Market Categories', 'Market Categories List'], inplace=True)\n",
    "\n",
    "        # Option 1: Apply transform() directly (drops columns)\n",
    "        data = loaded_filter.transform(data)\n",
    "    \n",
    "        # Option 2: Manually drop columns (if needed)\n",
    "        columns_to_drop_test = loaded_filter.get_columns_to_drop()\n",
    "        data = data.drop(columns=columns_to_drop_test, errors='ignore')\n",
    "\n",
    "        # Use the loaded reducer on new data\n",
    "        missing_original = set(loaded_reducer.original_categories) - set(data.columns)\n",
    "        for col in missing_original:\n",
    "            data[col] = 0\n",
    "        \n",
    "        # Now apply the reducer\n",
    "        data = loaded_reducer.transform(data)\n",
    "        \n",
    "        #\n",
    "        data = age_mode_col.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_ipo_transformer.transform(data)\n",
    "        \n",
    "        data.drop(columns=['Address (HQ)'], inplace=True)\n",
    "        #\n",
    "        data = loaded_employee_cleaner.transform(data)\n",
    "        # data.drop(columns=['Founders'], inplace=True)\n",
    "\n",
    "        # Process text with saved TF-IDF\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
    "            return text\n",
    "\n",
    "        data['Text_Combined'] = data['Text_Combined'].apply(clean_text)\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Final cleanup\n",
    "        data = pd.concat([data, category_dummies, tfidf_df], axis=1)\n",
    "\n",
    "        cols_to_drop = ['Market Categories', 'Tagline', 'Description', 'Text_Combined',\n",
    "                'Address (HQ)', 'Board Members', 'Founders']\n",
    "\n",
    "        existing_cols = [col for col in cols_to_drop if col in data.columns]\n",
    "        data.drop(existing_cols, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Similar functions for acquired and acquisitions...\n",
    "    # (Implementation would mirror the training preprocessing but using saved transformers)\n",
    "    \n",
    "\n",
    "    def process_acquisitions_new(data):\n",
    "        \"\"\"Process new acquisitions data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        \n",
    "        # Drop columns same as training\n",
    "        data.drop(columns=[\"Acquisition Profile\", \"News\", \"News Link\"], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        with open('custom_acquisitions_encoder.pkl', 'rb') as f:\n",
    "            encoder = pickle.load(f)\n",
    "        with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "            modes = pickle.load(f)\n",
    "\n",
    "        # Handle missing values with saved modes\n",
    "        for col, mode_val in modes.items():\n",
    "            if col in data.columns and mode_val is not None:\n",
    "                data[col].fillna(mode_val, inplace=True)\n",
    "        \n",
    "        # Convert date features same as training\n",
    "        if 'Deal announced on' in data.columns:\n",
    "            data['Deal_date'] = pd.to_datetime(data['Deal announced on'], dayfirst=True, errors='coerce')\n",
    "            data['Deal_day'] = data['Deal_date'].dt.day\n",
    "            data['Deal_month'] = data['Deal_date'].dt.month\n",
    "            data['Deal_dayofweek'] = data['Deal_date'].dt.dayofweek\n",
    "            data.drop(['Deal announced on', 'Deal_date'], axis=1, inplace=True)\n",
    "        \n",
    "        # Apply saved one-hot encoding\n",
    "        encoded_status_terms = encoder.transform(data)\n",
    "        data = pd.concat([data, encoded_status_terms], axis=1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process_acquired_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "\n",
    "        data = loaded_guesser.transform(data)\n",
    "\n",
    "        data = loaded_generalizer.transform(data)\n",
    "\n",
    "        data = loaded_filler.transform(data)\n",
    "\n",
    "        data = encoder.transform(data)\n",
    "        \n",
    "        data['Acquired by'].fillna('Salesforce', inplace=True)\n",
    "\n",
    "\n",
    "        columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter','Address (HQ)','API','Description',\n",
    "                           'Tagline','Market Categories','City (HQ)', 'State / Region (HQ)', 'Country (HQ)','Generalized Market Categories','Year Founded']\n",
    "        data.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Preprocess each new dataset\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path)) \n",
    "    acquisitions_new = process_acquisitions_new(pd.read_csv(acquisitions_path))\n",
    "    \n",
    "    def find_company_column(df):\n",
    "        for col in df.columns:\n",
    "            if \"company\" in col.lower():\n",
    "                return col\n",
    "        raise ValueError(\"No company name column found.\")\n",
    "\n",
    "    # Start with acquisition data\n",
    "    final_df = acquisitions_new.copy()\n",
    "\n",
    "    # Mapping for left_on column and its corresponding dataset\n",
    "    merge_targets = [\n",
    "        ('Acquired Company', acquired_new, '_Acquired'),\n",
    "        ('Acquiring Company', acquiring_new, '_Acquiring')\n",
    "    ]\n",
    "\n",
    "    # Perform merges in loop\n",
    "    for left_key, company_data, suffix in merge_targets:\n",
    "        company_col = find_company_column(company_data)\n",
    "\n",
    "        # Strip and lower case the company names for matching\n",
    "        final_df[left_key] = final_df[left_key].str.strip().str.lower()\n",
    "        company_data[company_col] = company_data[company_col].str.strip().str.lower()\n",
    "\n",
    "        # Merge\n",
    "        final_df = final_df.merge(\n",
    "            company_data,\n",
    "            how='left',\n",
    "            left_on=left_key,\n",
    "            right_on=company_col,\n",
    "            suffixes=('', suffix)\n",
    "        )\n",
    "\n",
    "        # Drop the extra company column if you want\n",
    "        final_df.drop(columns=[company_col], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    # Done!\n",
    "    print(final_df.head())\n",
    "    \n",
    "    \n",
    "    # Handle missing values with saved imputer\n",
    "    # final_new_imputed = pd.DataFrame(\n",
    "    #     final_imputer.transform(final_df),\n",
    "    #     columns=final_new.columns\n",
    "    # )\n",
    "\n",
    "    with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "        modes = pickle.load(f)\n",
    "\n",
    "    for col, mode_val in modes.items():\n",
    "       # if col in df.columns and mode_val is not None:\n",
    "           final_df[col].fillna(mode_val, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Prepare features\n",
    "    # X_new = final_df.drop(\n",
    "    #     ['Deal size class', 'Acquired Company', 'Acquiring Company'], \n",
    "    #     axis=1, errors='ignore'\n",
    "    # )\n",
    "    with open('selected_features.pkl', 'rb') as f:\n",
    "        selected_features = pickle.load(f)\n",
    "\n",
    "    # Filter the test data to keep only those features\n",
    "    X_new = final_df[selected_features]\n",
    "    # Apply same scaling and PCA as training\n",
    "    X_new_scaled = final_scaler.transform(X_new)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Save predictions with original data\n",
    "    final_new['Predicted_Deal_Size'] = predictions\n",
    "    final_new.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def browse_file(entry_widget):\n",
    "    directory = filedialog.askopenfilename()\n",
    "    if directory:\n",
    "        entry_widget.delete(0, tk.END)\n",
    "        entry_widget.insert(0, directory)\n",
    "\n",
    "def predict_data():\n",
    "    # Get input values from entries\n",
    "    paths = [\n",
    "        entry1.get(),\n",
    "        entry2.get(),\n",
    "        entry3.get()\n",
    "    ]\n",
    "    \n",
    "    # Validate paths\n",
    "    for path in paths:\n",
    "        if not path.strip():\n",
    "            messagebox.showerror(\"Error\", \"All paths must be selected!\")\n",
    "            return\n",
    "    \n",
    "    # Call your prediction function\n",
    "    try:\n",
    "        predict_new_data(*paths, \"new_predictions.csv\")\n",
    "        messagebox.showinfo(\"Success\", \"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Predictor\")\n",
    "root.geometry(\"1200x800\")\n",
    "\n",
    "# Function to create consistent input fields with browse buttons\n",
    "def create_file_input(row, label_text):\n",
    "    # Label\n",
    "    tk.Label(root, text=label_text).grid(row=row, column=0, padx=10, pady=5, sticky=tk.W)\n",
    "    \n",
    "    # Entry field\n",
    "    entry = tk.Entry(root, width=80)\n",
    "    entry.grid(row=row, column=1, padx=10, pady=5)\n",
    "    \n",
    "    # Browse button\n",
    "    browse_btn = tk.Button(root, text=\"Browse\", \n",
    "                          command=lambda: browse_file(entry))\n",
    "    browse_btn.grid(row=row, column=2, padx=10, pady=5)\n",
    "    \n",
    "    return entry\n",
    "\n",
    "# Create input fields with browse buttons\n",
    "entry1 = create_file_input(0, \"Acquiring Path:\")\n",
    "entry2 = create_file_input(1, \"Acquired Path:\")\n",
    "entry3 = create_file_input(2, \"Acquisitions Path:\")\n",
    "\n",
    "# Prediction button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_data,\n",
    "                          bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "predict_button.grid(row=3, column=0, columnspan=3, pady=20, ipadx=10, ipady=5)\n",
    "\n",
    "# Configure grid layout\n",
    "root.grid_columnconfigure(1, weight=1)  # Make entry fields expandable\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
