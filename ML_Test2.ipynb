{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698874ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMPORTS ========================\n",
    "from collections import Counter\n",
    "from joblib import dump\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pycountry\n",
    "import re\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117071b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================== CLASSES ========================\n",
    "class CorrelationFilter:\n",
    "\n",
    "    def __init__(self, threshold=0.85):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = set()\n",
    "\n",
    "    def fit(self, data):\n",
    "        numerical_data = data.select_dtypes(include=[np.number])\n",
    "        corr_matrix = numerical_data.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    self.to_drop.add(colname)\n",
    "\n",
    "    def transform(self, data):\n",
    "        return data.drop(columns=list(self.to_drop & set(data.columns)), errors='ignore')\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class CategoryReducer:\n",
    "    def __init__(self, category_columns, top_n=15):\n",
    "        self.category_columns = category_columns\n",
    "        self.top_n = top_n\n",
    "        self.top_categories = None\n",
    "        \n",
    "        # Backward compatibility attributes\n",
    "        if not hasattr(self, 'original_categories'):\n",
    "            self.original_categories = category_columns\n",
    "        if not hasattr(self, 'expected_columns'):\n",
    "            self.expected_columns = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Store original columns from training data\n",
    "        self.original_categories = list(data[self.category_columns].columns)\n",
    "        \n",
    "        # Find top categories\n",
    "        self.top_categories = (\n",
    "            data[self.category_columns].sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(self.top_n)\n",
    "            .index.tolist()\n",
    "        )\n",
    "        \n",
    "        # Store expected output columns\n",
    "        self.expected_columns = self.top_categories + ['Other']\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Backward compatibility for old models\n",
    "        if not hasattr(self, 'original_categories'):\n",
    "            self.original_categories = self.category_columns\n",
    "        if not hasattr(self, 'expected_columns'):\n",
    "            self.expected_columns = self.top_categories + ['Other']\n",
    "\n",
    "        # Add missing original columns\n",
    "        missing_cols = set(self.original_categories) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data[col] = 0\n",
    "\n",
    "        # Add missing top categories\n",
    "        missing_top = set(self.top_categories) - set(data.columns)\n",
    "        for col in missing_top:\n",
    "            data[col] = 0\n",
    "\n",
    "        # Create Other column\n",
    "        other_cols = [c for c in self.original_categories \n",
    "                     if c not in self.top_categories]\n",
    "        data['Other'] = data[other_cols].sum(axis=1).apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "        # Return only expected columns\n",
    "        return data[self.expected_columns].copy()\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "    # Handle pickle compatibility\n",
    "    def __getstate__(self):\n",
    "        return {k: v for k, v in self.__dict__.items() \n",
    "                if k in ['category_columns', 'top_n', 'top_categories',\n",
    "                         'original_categories', 'expected_columns']}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        # Initialize missing attributes for old versions\n",
    "        if not hasattr(self, 'original_categories'):\n",
    "            self.original_categories = self.category_columns\n",
    "        if not hasattr(self, 'expected_columns'):\n",
    "            self.expected_columns = (self.top_categories + ['Other'] \n",
    "                                     if self.top_categories else None)\n",
    "class AgeTransformer:\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "        self.age_mode = None\n",
    "        self.column_exists = True  # Track if column existed during training\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Check if column exists in training data\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            self.column_exists = False\n",
    "            return self\n",
    "            \n",
    "        data = data.copy()\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        mode_series = data['age'].mode()\n",
    "        self.age_mode = mode_series[0] if not mode_series.empty else 5\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Handle missing 'Year Founded' column gracefully\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Create column if it doesn't exist\n",
    "        if 'Year Founded' not in data.columns:\n",
    "            if self.column_exists:\n",
    "                # Column existed in training but missing in new data\n",
    "                data['Year Founded'] = np.nan\n",
    "            else:\n",
    "                # Column never existed (new scenario)\n",
    "                data['age'] = self.age_mode\n",
    "                return data\n",
    "                \n",
    "        # Original processing if column exists\n",
    "        data['Year Founded'] = pd.to_numeric(data['Year Founded'], errors='coerce')\n",
    "        data['age'] = self.current_year - data['Year Founded']\n",
    "        data.drop(columns=['Year Founded'], inplace=True, errors='ignore')\n",
    "        data['age'].fillna(self.age_mode, inplace=True)\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "class IPOAgeTransformer:\n",
    "\n",
    "    def __init__(self, current_year=2025):\n",
    "        self.current_year = current_year\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['IPO'] = pd.to_numeric(df['IPO'], errors='coerce')\n",
    "        df['age IPO'] = self.current_year - df['IPO']\n",
    "        df['age IPO'].replace(np.nan, -1, inplace=True)\n",
    "        if 'IPO' in df.columns:\n",
    "            df.drop(columns=['IPO'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class EmployeeDataCleaner:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.employee_mode = None\n",
    "        self.mean_without_zeros = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.employee_mode = data['Number of Employees (year of last update)'].mode()[0] if not data['Number of Employees (year of last update)'].mode().empty else 0\n",
    "        self.mean_without_zeros = data.loc[data['Number of Employees'] > 0, 'Number of Employees'].mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        if 'Number of Employees (year of last update)' in df.columns:\n",
    "            df['Number of Employees (year of last update)'].fillna(self.employee_mode, inplace=True)\n",
    "        if 'Number of Employees' in df.columns:\n",
    "            df['Number of Employees'] = df['Number of Employees'].apply(lambda x: 0 if pd.isna(x) or x < 0 else x)\n",
    "            df['Number of Employees'] = df['Number of Employees'].replace(0, self.mean_without_zeros)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class TaglineCategoryGuesser:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_keywords = {'Artificial Intelligence': ['ai', 'machine learning', 'deep learning', 'neural network'], 'Mobile': ['mobile', 'android', 'ios', 'app store', 'smartphone'], 'E-Commerce': ['ecommerce', 'e-commerce', 'shopping', 'online store'], 'FinTech': ['finance', 'banking', 'payments', 'fintech', 'crypto', 'blockchain'], 'Healthcare': ['health', 'medical', 'hospital', 'doctor', 'pharma'], 'Social Media': ['social network', 'community', 'messaging', 'chat'], 'Gaming': ['game', 'gaming', 'video game', 'esports'], 'Cloud': ['cloud', 'saas', 'paas', 'infrastructure'], 'EdTech': ['education', 'learning', 'students', 'teaching', 'school'], 'Data Analytics': ['analytics', 'data science', 'big data', 'insights']}\n",
    "\n",
    "    def guess_category_from_tagline(self, tagline):\n",
    "        tagline = str(tagline).lower()\n",
    "        matched = [cat for (cat, keywords) in self.category_keywords.items() if any((keyword in tagline for keyword in keywords))]\n",
    "        if len(matched) == 0:\n",
    "            matched = ['Software', 'Advertising']\n",
    "        elif len(matched) == 1:\n",
    "            matched.append('Software')\n",
    "        return ', '.join(matched)\n",
    "\n",
    "    def fit(self, data):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        df = data.copy()\n",
    "        df['Tagline'] = df['Tagline'].fillna('')\n",
    "        df['Market Categories'] = df['Market Categories'].fillna('Unknown')\n",
    "        df['Market Categories'] = df.apply(lambda row: self.guess_category_from_tagline(row['Tagline']) if str(row['Market Categories']).strip().lower() in ['unknown', 'nan', 'none', ''] else row['Market Categories'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        return self.fit(data).transform(data)\n",
    "class MarketCategoryGeneralizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.category_mapping = {'Software': 'Technology & Software', 'Advertising': 'Advertising & Marketing', 'E-Commerce': 'E-Commerce & Online Services', 'Mobile': 'Mobile & Consumer Electronics', 'Games': 'Games & Entertainment', 'Social Media': 'Social Networking & Communication', 'Cloud': 'Technology & Software', 'Finance': 'Finance & Payments', 'Healthcare': 'Healthcare & Wellness', 'Semiconductors': 'Technology Hardware', 'Data Analytics': 'Analytics & Data Science', 'Search': 'Advertising & Marketing', 'Video': 'Games & Entertainment', 'Networking': 'Telecom & Networks', 'Messaging': 'Social Networking & Communication', 'Education': 'Education & Learning', 'News': 'Media & News', 'Photo Sharing': 'Digital Media & Content', 'Mobile Payments': 'Finance & Payments', 'Robotics': 'Games & Entertainment', 'Music': 'Games & Entertainment', 'Photo Editing': 'Digital Media & Content', 'Online Rental': 'E-Commerce & Online Services', 'Location Based Services': 'Telecom & Networks', 'Enterprise Software': 'Technology & Software', 'Video Streaming': 'Games & Entertainment', 'PaaS': 'Technology & Software', 'SaaS': 'Technology & Software', 'Health and Wellness': 'Healthcare & Wellness', 'Web Hosting': 'Technology & Software', 'Internet of Things': 'IoT (Internet of Things)', 'Cloud Security': 'Technology & Software', 'Virtual Currency': 'Finance & Payments', 'Search Marketing': 'Advertising & Marketing', 'Mobile Social': 'Social Networking & Communication', 'Retail': 'Retail & Fashion', 'Consulting': 'Others & Miscellaneous', 'Aerospace': 'Others & Miscellaneous', 'Food Delivery': 'Consumer Goods & Services', 'Fashion': 'Retail & Fashion', 'Wine And Spirits': 'Consumer Goods & Services', 'Streaming': 'Games & Entertainment', 'Task Management': 'Others & Miscellaneous', 'Video Chat': 'Social Networking & Communication', 'Personalization': 'Advertising & Marketing', 'Shopping': 'E-Commerce & Online Services', 'Local': 'E-Commerce & Online Services', 'News': 'Media & News', 'Fraud Detection': 'Advertising & Marketing', 'Image Recognition': 'Technology Hardware', 'Virtualization': 'Games & Entertainment', 'Analytics': 'Analytics & Data Science', 'Video on Demand': 'Games & Entertainment', 'Mobile Payments': 'Finance & Payments', 'Marketing Automation': 'Advertising & Marketing', 'Consumer Electronics': 'Mobile & Consumer Electronics', 'Video Games': 'Games & Entertainment', 'Public Relations': 'Advertising & Marketing'}\n",
    "\n",
    "    def map_categories(self, row):\n",
    "        categories = str(row).split(',')\n",
    "        generalized = []\n",
    "        for cat in categories:\n",
    "            cat = cat.strip()\n",
    "            if cat in self.category_mapping:\n",
    "                generalized.append(self.category_mapping[cat])\n",
    "            else:\n",
    "                generalized.append('Others & Miscellaneous')\n",
    "        return ', '.join(set(generalized))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        df['Generalized Market Categories'] = df['Market Categories'].fillna('').apply(self.map_categories)\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CountryRegionFiller:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.countries = [country.name for country in pycountry.countries]\n",
    "        self.regions = ['California', 'New York', 'Texas', 'Basel', 'Utah', 'ÃŽle-de-France', 'Bavaria', 'Ontario', 'Switzerland', 'United States', 'France', 'Great Britain', 'Israel', 'Sweden', 'Canada', 'Germany', 'Japan', 'India', 'Denmark', 'China', 'Spain', 'Netherlands', 'Finland', 'Australia', 'Ireland', 'United Stats of AMerica', 'United Arab Emirates', 'Quebec']\n",
    "\n",
    "    def find_place(self, text, place_list):\n",
    "        for place in place_list:\n",
    "            if re.search('\\\\b' + re.escape(place) + '\\\\b', str(text)):\n",
    "                return place\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for (idx, row) in df[df['Country (HQ)'].isnull() | df['State / Region (HQ)'].isnull()].iterrows():\n",
    "            desc = row['Description']\n",
    "            if pd.isnull(desc):\n",
    "                continue\n",
    "            country = self.find_place(desc, self.countries)\n",
    "            region = self.find_place(desc, self.regions)\n",
    "            if pd.isnull(row['Country (HQ)']) and country:\n",
    "                df.at[idx, 'Country (HQ)'] = country\n",
    "            if pd.isnull(row['State / Region (HQ)']) and region:\n",
    "                df.at[idx, 'State / Region (HQ)'] = region\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "class CategoricalFillerAndEncoder:\n",
    "\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.modes = {}\n",
    "        self.label_encoders = {}\n",
    "        self.label_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            mode_val = X[col].mode()[0]\n",
    "            self.modes[col] = mode_val\n",
    "            le = LabelEncoder()\n",
    "            filled = X[col].fillna(mode_val).astype(str)\n",
    "            le.fit(filled)\n",
    "            self.label_encoders[col] = le\n",
    "            self.label_maps[col] = {label: i for (i, label) in enumerate(le.classes_)}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for col in self.columns:\n",
    "            mode_val = self.modes[col]\n",
    "            label_map = self.label_maps[col]\n",
    "            df[col] = df[col].fillna(mode_val).astype(str)\n",
    "            df[col + '_LabelEncoded'] = df[col].map(lambda x: label_map.get(x, -1))\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.status_cols_ = None\n",
    "        self.terms_cols_ = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.status_cols_ = df['Status'].unique()\n",
    "        self.terms_cols_ = df['Terms'].unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df = pd.get_dummies(df, columns=['Status'], drop_first=False)\n",
    "        df = pd.get_dummies(df, columns=['Terms'], drop_first=False)\n",
    "        if 'Terms_Cash, Stock' in df.columns:\n",
    "            cash_stock_mask = df['Terms_Cash, Stock'] == 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Cash'] = 1\n",
    "            df.loc[cash_stock_mask, 'Terms_Stock'] = 1\n",
    "            df = df.drop('Terms_Cash, Stock', axis=1)\n",
    "        expected_cols = [f'Status_{s}' for s in self.status_cols_] + [f'Terms_{t}' for t in self.terms_cols_ if t != 'Cash, Stock']\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        return df[expected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf75f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kero/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) [' Advertising Platforms', ' All Markets', ' All Students', ' Auctions', ' Blogging Platforms', ' Cloud Computing', ' Colleges', ' Communities', ' Consulting', ' Consumer Electronics', ' Consumer Goods', ' Content Creators', ' Creative', ' Crowdsourcing', ' Curated Web', ' Design', ' Digital Media', ' E-Commerce', ' Email', ' Enterprise Software', ' Enterprises', ' Facebook Applications', ' Hardware', ' Hardware + Software', ' Identity', ' Image Recognition', ' Information Technology', ' MicroBlogging', ' Mobile', ' Networking', ' Photography', ' RIM', ' SMS', ' Search', ' Security', ' Social Bookmarking', ' Social Media', ' Software', ' Storage', ' Technology', ' Video Streaming', ' Web Hosting', ' Wireless'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AgeTransformer' object has no attribute 'column_exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 263\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[43mpredict_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquiring.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquired_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquired.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43macquisitions_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_acquisitions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_predictions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 227\u001b[0m, in \u001b[0;36mpredict_new_data\u001b[0;34m(acquiring_path, acquired_path, acquisitions_path, output_path)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data \n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Preprocess each new dataset\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m acquiring_new \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_acquiring_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43macquiring_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m acquired_new \u001b[38;5;241m=\u001b[39m process_acquired_new(pd\u001b[38;5;241m.\u001b[39mread_csv(acquired_path)) \n\u001b[1;32m    229\u001b[0m acquisitions_new \u001b[38;5;241m=\u001b[39m process_acquisitions_new(pd\u001b[38;5;241m.\u001b[39mread_csv(acquisitions_path))\n",
      "Cell \u001b[0;32mIn[3], line 132\u001b[0m, in \u001b[0;36mpredict_new_data.<locals>.process_acquiring_new\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    129\u001b[0m data \u001b[38;5;241m=\u001b[39m loaded_reducer\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mage_mode_col\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    135\u001b[0m data \u001b[38;5;241m=\u001b[39m loaded_ipo_transformer\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m, in \u001b[0;36mAgeTransformer.transform\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Create column if it doesn't exist\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear Founded\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_exists\u001b[49m:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# Column existed in training but missing in new data\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear Founded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;66;03m# Column never existed (new scenario)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AgeTransformer' object has no attribute 'column_exists'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_new_data(acquiring_path, acquired_path, acquisitions_path, output_path):\n",
    "    \"\"\"\n",
    "    Process new data and make predictions using saved models\n",
    "    \n",
    "    Args:\n",
    "        acquiring_path: Path to new acquiring companies CSV\n",
    "        acquired_path: Path to new acquired companies CSV  \n",
    "        acquisitions_path: Path to new acquisitions CSV\n",
    "        output_path: Where to save predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===================================\n",
    "    # Load All Saved Preprocessing Objects\n",
    "    # ===================================\n",
    "    \n",
    "    # Load acquiring company transformers\n",
    "    with open('mlb_acquiring.pkl', 'rb') as f:\n",
    "        mlb_acquiring = pickle.load(f)\n",
    "    \n",
    "    with open(\"correlation_filte_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_filter = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    with open(\"category_reducer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_reducer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open('Age_column_acquiring.pkl', 'rb') as f:\n",
    "        age_mode_col =  pickle.load(f)\n",
    "    #\n",
    "    with open(\"ipo_transformer_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_ipo_transformer = pickle.load(f)\n",
    "    #\n",
    "    with open(\"employee_cleaner_acquiring.pkl\", \"rb\") as f:\n",
    "        loaded_employee_cleaner = pickle.load(f)\n",
    "\n",
    "\n",
    "    with open('tfidf_acquiring.pkl', 'rb') as f:\n",
    "        tfidf_acquiring = pickle.load(f)\n",
    "        \n",
    "    #\n",
    "    with open(\"tagline_guesser_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_guesser = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"category_generalizer_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_generalizer = pickle.load(f)\n",
    "\n",
    "    #\n",
    "    with open(\"country_region_filler_acquired.pkl\", \"rb\") as f:\n",
    "        loaded_filler = pickle.load(f)\n",
    "    \n",
    "    #\n",
    "    with open(\"categorical_encoder_acquired.pkl\", \"rb\") as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    # Load acquired company transformers\n",
    "    with open('mlb_acquired.pkl', 'rb') as f:\n",
    "        mlb_acquired = pickle.load(f)\n",
    "    # with open('label_encoders_acquired.pkl', 'rb') as f:\n",
    "    #     label_encoders_acquired = pickle.load(f)\n",
    "        \n",
    "    # Load acquisitions transformers\n",
    "    # with open('ohe_acquisitions.pkl', 'rb') as f:\n",
    "    #     ohe_acquisitions = pickle.load(f)\n",
    "        \n",
    "    # Load final preprocessing objects\n",
    "    # with open('final_imputer.pkl', 'rb') as f:\n",
    "    #     final_imputer = pickle.load(f)\n",
    "    with open('final_scaler.pkl', 'rb') as f:\n",
    "        final_scaler = pickle.load(f)\n",
    "    with open('final_pca.pkl', 'rb') as f:\n",
    "        final_pca = pickle.load(f)\n",
    "    with open('target_encoder.pkl', 'rb') as f:\n",
    "        target_encoder = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Load saved preprocessing objects\n",
    "    \n",
    "    # Load model\n",
    "    # with open('final_model.pkl', 'rb') as f:\n",
    "    #     model = pickle.load(f)\n",
    "    \n",
    "    # ==============================\n",
    "    # Preprocess New Data (Same as Training)\n",
    "    # ==============================\n",
    "    \n",
    "    # Process each dataset with saved transformers\n",
    "    def process_acquiring_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Apply same cleaning as training\n",
    "        data.drop(['CrunchBase Profile','Image','Homepage','Twitter','API'], \n",
    "                 axis=1, inplace=True, errors='ignore')\n",
    "                \n",
    "        data['Number of Employees'] = data['Number of Employees'].replace({',': ''}, regex=True)\n",
    "        data['Number of Employees'] = data['Number of Employees'].fillna(0).astype(int)\n",
    "        \n",
    "        # Use mean from training (would need to save this)\n",
    "        data['Number of Employees'] = data['Number of Employees'].replace(\n",
    "            0, 450)  # Replace with saved mean from training\n",
    "        \n",
    "        # Handle IPO status\n",
    "        data['IPO'] = data['IPO'].replace(\"Not yet\", np.nan)\n",
    "        data['Is_Public'] = data['IPO'].notna().astype(int)\n",
    "        data.drop('IPO', axis=1, inplace=True)\n",
    "\n",
    "        # Process Market Categories with saved MLB\n",
    "        data['Market Categories'] = data['Market Categories'].fillna('')\n",
    "        category_dummies = pd.DataFrame(\n",
    "            mlb_acquiring.transform(data['Market Categories'].str.split(',')),\n",
    "            columns=mlb_acquiring.classes_,\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        #\n",
    "        data = loaded_filter.transform(data)\n",
    "\n",
    "\n",
    "        # Use the loaded reducer on new data\n",
    "        missing_original = set(loaded_reducer.original_categories) - set(data.columns)\n",
    "        for col in missing_original:\n",
    "            data[col] = 0\n",
    "        \n",
    "        # Now apply the reducer\n",
    "        data = loaded_reducer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = age_mode_col.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_ipo_transformer.transform(data)\n",
    "\n",
    "        #\n",
    "        data = loaded_employee_cleaner.transform(data)\n",
    "\n",
    "\n",
    "\n",
    "        # Process text with saved TF-IDF\n",
    "        data['Text_Combined'] = data['Tagline'].fillna('') + ' ' + data['Description'].fillna('')\n",
    "        def clean_text(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
    "            return text\n",
    "\n",
    "        data['Text_Combined'] = data['Text_Combined'].apply(clean_text)\n",
    "        tfidf_features = tfidf_acquiring.transform(data['Text_Combined'])\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_features.toarray(), \n",
    "            columns=tfidf_acquiring.get_feature_names_out(),\n",
    "            index=data.index\n",
    "        )\n",
    "        \n",
    "        # Final cleanup\n",
    "        data = pd.concat([data, category_dummies, tfidf_df], axis=1)\n",
    "        data.drop(['Market Categories', 'Tagline', 'Description', 'Text_Combined','Address (HQ)',\n",
    "                   'Board Members','Founders'], \n",
    "                 axis=1, inplace=True)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Similar functions for acquired and acquisitions...\n",
    "    # (Implementation would mirror the training preprocessing but using saved transformers)\n",
    "    \n",
    "\n",
    "    def process_acquisitions_new(data):\n",
    "        \"\"\"Process new acquisitions data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        \n",
    "        # Drop columns same as training\n",
    "        data.drop(columns=[\"Acquisition Profile\", \"News\", \"News Link\"], \n",
    "                inplace=True, errors='ignore')\n",
    "        \n",
    "        with open('custom_acquisitions_encoder.pkl', 'rb') as f:\n",
    "            encoder = pickle.load(f)\n",
    "        with open('mode_acquisitions_imputer.pkl', 'rb') as f:\n",
    "            modes = pickle.load(f)\n",
    "\n",
    "        # Handle missing values with saved modes\n",
    "        for col, mode_val in modes.items():\n",
    "            if col in data.columns and mode_val is not None:\n",
    "                data[col].fillna(mode_val, inplace=True)\n",
    "        \n",
    "        # Convert date features same as training\n",
    "        if 'Deal announced on' in data.columns:\n",
    "            data['Deal_date'] = pd.to_datetime(data['Deal announced on'], dayfirst=True, errors='coerce')\n",
    "            data['Deal_day'] = data['Deal_date'].dt.day\n",
    "            data['Deal_month'] = data['Deal_date'].dt.month\n",
    "            data['Deal_dayofweek'] = data['Deal_date'].dt.dayofweek\n",
    "            data.drop(['Deal announced on', 'Deal_date'], axis=1, inplace=True)\n",
    "        \n",
    "        # Apply saved one-hot encoding\n",
    "        encoded_status_terms = encoder.transform(data)\n",
    "        data = pd.concat([data, encoded_status_terms], axis=1)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def process_acquired_new(data):\n",
    "        \"\"\"Process new acquiring data with saved transformers\"\"\"\n",
    "        data = data.copy()\n",
    "\n",
    "        data = loaded_guesser.transform(data)\n",
    "\n",
    "        data = loaded_generalizer.transform(data)\n",
    "\n",
    "        data = loaded_filler.transform(data)\n",
    "\n",
    "        data = encoder.transform(data)\n",
    "        \n",
    "        data['Acquired by'].fillna('Salesforce', inplace=True)\n",
    "\n",
    "\n",
    "        columns_to_drop = ['Image', 'CrunchBase Profile', 'Homepage', 'Twitter','Address (HQ)','API','Description',\n",
    "                           'Tagline','Market Categories','City (HQ)', 'State / Region (HQ)', 'Country (HQ)','Generalized Market Categories','Year Founded']\n",
    "        data.drop(columns=columns_to_drop, inplace=True)\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Preprocess each new dataset\n",
    "    acquiring_new = process_acquiring_new(pd.read_csv(acquiring_path))\n",
    "    acquired_new = process_acquired_new(pd.read_csv(acquired_path)) \n",
    "    acquisitions_new = process_acquisitions_new(pd.read_csv(acquisitions_path))\n",
    "    \n",
    "    # Merge the new data (same as training)\n",
    "    final_new = merge_datasets(\n",
    "        acquiring_new, acquired_new, acquisitions_new, save_artifacts=False\n",
    "    )\n",
    "    \n",
    "    # Handle missing values with saved imputer\n",
    "    final_new_imputed = pd.DataFrame(\n",
    "        final_imputer.transform(final_new),\n",
    "        columns=final_new.columns\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    X_new = final_new_imputed.drop(\n",
    "        ['Deal size class', 'Acquired Company', 'Acquiring Company'], \n",
    "        axis=1, errors='ignore'\n",
    "    )\n",
    "    \n",
    "    # Apply same scaling and PCA as training\n",
    "    X_new_scaled = final_scaler.transform(X_new)\n",
    "    X_new_pca = final_pca.transform(X_new_scaled)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(X_new_pca)\n",
    "    predictions = target_encoder.inverse_transform(predictions_encoded)\n",
    "    \n",
    "    # Save predictions with original data\n",
    "    final_new['Predicted_Deal_Size'] = predictions\n",
    "    final_new.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_new_data(\n",
    "        acquiring_path=\"new_acquiring.csv\",\n",
    "        acquired_path=\"new_acquired.csv\",\n",
    "        acquisitions_path=\"new_acquisitions.csv\",\n",
    "        output_path=\"new_predictions.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e293cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "def browse_file(entry_widget):\n",
    "    directory = filedialog.askopenfilename()\n",
    "    if directory:\n",
    "        entry_widget.delete(0, tk.END)\n",
    "        entry_widget.insert(0, directory)\n",
    "\n",
    "def predict_data():\n",
    "    # Get input values from entries\n",
    "    paths = [\n",
    "        entry1.get(),\n",
    "        entry2.get(),\n",
    "        entry3.get()\n",
    "    ]\n",
    "    \n",
    "    # Validate paths\n",
    "    for path in paths:\n",
    "        if not path.strip():\n",
    "            messagebox.showerror(\"Error\", \"All paths must be selected!\")\n",
    "            return\n",
    "    \n",
    "    # Call your prediction function\n",
    "    try:\n",
    "        predict_new_data(*paths, \"new_predictions.csv\")\n",
    "        messagebox.showinfo(\"Success\", \"Prediction completed successfully!\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Predictor\")\n",
    "root.geometry(\"1200x800\")\n",
    "\n",
    "# Function to create consistent input fields with browse buttons\n",
    "def create_file_input(row, label_text):\n",
    "    # Label\n",
    "    tk.Label(root, text=label_text).grid(row=row, column=0, padx=10, pady=5, sticky=tk.W)\n",
    "    \n",
    "    # Entry field\n",
    "    entry = tk.Entry(root, width=80)\n",
    "    entry.grid(row=row, column=1, padx=10, pady=5)\n",
    "    \n",
    "    # Browse button\n",
    "    browse_btn = tk.Button(root, text=\"Browse\", \n",
    "                          command=lambda: browse_file(entry))\n",
    "    browse_btn.grid(row=row, column=2, padx=10, pady=5)\n",
    "    \n",
    "    return entry\n",
    "\n",
    "# Create input fields with browse buttons\n",
    "entry1 = create_file_input(0, \"Acquiring Path:\")\n",
    "entry2 = create_file_input(1, \"Acquired Path:\")\n",
    "entry3 = create_file_input(2, \"Acquisitions Path:\")\n",
    "\n",
    "# Prediction button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_data,\n",
    "                          bg=\"#4CAF50\", fg=\"white\", font=(\"Arial\", 12))\n",
    "predict_button.grid(row=3, column=0, columnspan=3, pady=20, ipadx=10, ipady=5)\n",
    "\n",
    "# Configure grid layout\n",
    "root.grid_columnconfigure(1, weight=1)  # Make entry fields expandable\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
